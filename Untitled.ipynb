{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a291b93d-190c-4f90-a718-518e96ac54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/merged_pubchem_chembl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d0948-b6a3-4e20-8e6e-d2469dea087d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593424e-72f5-4eaa-8b9a-0eeaeb3fc4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b201f923-f189-48b0-9b78-13da1797758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë¹ ë¥¸ ìƒê´€ê´€ê³„ ê°œì„  ì‹œì‘!\n",
      "ğŸ§ª ë¶„ì ê¸°ìˆ ì ê³„ì‚°...\n",
      "ë°ì´í„° í¬ê¸°: 1960\n",
      "ğŸ§¹ ì•„ì›ƒë¼ì´ì–´ ì œê±°...\n",
      "ì•„ì›ƒë¼ì´ì–´ ì œê±°: 1960 â†’ 1959\n",
      "ğŸ“Š ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬...\n",
      "ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ ìµœì í™”...\n",
      "ğŸ¤– ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨...\n",
      "ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "  rf í›ˆë ¨ ì¤‘...\n",
      "  xgb í›ˆë ¨ ì¤‘...\n",
      "  lgb í›ˆë ¨ ì¤‘...\n",
      "  gb í›ˆë ¨ ì¤‘...\n",
      "  et í›ˆë ¨ ì¤‘...\n",
      "\n",
      "ğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:\n",
      "  rf (ë³€í™˜): 0.4054 (A=1.000, B=0.676)\n",
      "  xgb (ë³€í™˜): 0.3889 (A=1.000, B=0.648)\n",
      "  lgb (ë³€í™˜): 0.3973 (A=1.000, B=0.662)\n",
      "  gb (ë³€í™˜): 0.3863 (A=1.000, B=0.644)\n",
      "  et (ë³€í™˜): 0.4092 (A=1.000, B=0.682)\n",
      "\n",
      "ğŸ­ ìƒê´€ê´€ê³„ ìµœì í™” ì•™ìƒë¸”...\n",
      "ì„ íƒëœ ëª¨ë¸: ['et', 'rf', 'lgb', 'xgb']\n",
      "ìµœì  ê°€ì¤‘ì¹˜: {'et': np.float64(0.7468461281871787), 'rf': np.float64(0.008466055649200789), 'lgb': np.float64(0.2446878161636207), 'xgb': np.float64(0.0)}\n",
      "\n",
      "ğŸ‰ ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥:\n",
      "   Competition Score: 0.4104\n",
      "   RMSE: 0.8591\n",
      "   A (Normalized RMSE): 1.0000\n",
      "   B (CorrelationÂ²): 0.6840\n",
      "   pIC50 ìƒê´€ê´€ê³„: 0.8271 (p=1.308e-99)\n",
      "\n",
      "ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ìˆ˜ì •)...\n",
      "íŠ¹ì„± ìˆœì„œ: ['MolWt', 'MolWt', 'LogP', 'LogP', 'TPSA', 'TPSA', 'NumRotatableBonds', 'NumRotatableBonds', 'NumHAcceptors', 'NumHAcceptors', 'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT', 'source_chembl', 'source_pubchem']\n",
      "ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°: 127/127\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: (127, 17)\n",
      "í›ˆë ¨ ë°ì´í„° í˜•íƒœ: (1567, 17)\n",
      "âœ… íŠ¹ì„± ìˆœì„œ ì¼ì¹˜\n",
      "ğŸ”® ëª¨ë¸ë³„ ì˜ˆì¸¡ ì¤‘...\n",
      "  et: ê°€ì¤‘ì¹˜ 0.7468\n",
      "    et: ë³€í™˜ ëª¨ë¸ ì‚¬ìš©\n",
      "  rf: ê°€ì¤‘ì¹˜ 0.0085\n",
      "    rf: ë³€í™˜ ëª¨ë¸ ì‚¬ìš©\n",
      "  lgb: ê°€ì¤‘ì¹˜ 0.2447\n",
      "    lgb: ë³€í™˜ ëª¨ë¸ ì‚¬ìš©\n",
      "  xgb: ê°€ì¤‘ì¹˜ 0.0000\n",
      "    xgb: ë³€í™˜ ëª¨ë¸ ì‚¬ìš©\n",
      "\n",
      "ğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:\n",
      "  log_IC50 ë²”ìœ„: -7.871 ~ -5.008\n",
      "  IC50 (nM) ë²”ìœ„: 0.0 ~ 9.8\n",
      "  IC50 (nM) ì¤‘ê°„ê°’: 0.7\n",
      "\n",
      "âœ… fixed_correlation_submission.csv ìƒì„±!\n",
      "ğŸ“Š ì˜ˆì¸¡ê°’ ìˆ˜: 127\n",
      "\n",
      "==================================================\n",
      "ğŸ† ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
      "==================================================\n",
      "ê²€ì¦ Competition Score: 0.4104\n",
      "A (Normalized RMSE): 1.0000\n",
      "B (CorrelationÂ²): 0.6840\n",
      "pIC50 ìƒê´€ê´€ê³„: 0.8271\n",
      "ì„±ëŠ¥ ê°œì„ : 0.0104 (ê¸°ì¤€ 0.4 ëŒ€ë¹„)\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ ê¸°ì—¬ë„:\n",
      "  et: 74.7%\n",
      "  rf: 0.8%\n",
      "  lgb: 24.5%\n",
      "  xgb: 0.0%\n",
      "==================================================\n",
      "âœ… ì‘ì—… ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ë¹ ë¥¸ ìƒê´€ê´€ê³„ ê°œì„  ì „ëµ (ê¸°ì¡´ ì½”ë“œì— ì¶”ê°€)\n",
    "- íƒ€ê²Ÿ ë³€í™˜ ìµœì í™”\n",
    "- ì•„ì›ƒë¼ì´ì–´ ì œê±°\n",
    "- íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ê°œì„ \n",
    "- ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compute_safe_descriptors(smiles):\n",
    "    \"\"\"ê¸°ì¡´ê³¼ ë™ì¼í•œ ì•ˆì „í•œ ê¸°ìˆ ì ê³„ì‚°\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan] * 10\n",
    "    \n",
    "    try:\n",
    "        return [\n",
    "            Descriptors.MolWt(mol),\n",
    "            Descriptors.MolLogP(mol),\n",
    "            Descriptors.TPSA(mol),\n",
    "            Descriptors.NumRotatableBonds(mol),\n",
    "            Descriptors.NumHAcceptors(mol),\n",
    "            Descriptors.NumHDonors(mol),\n",
    "            Descriptors.NumAromaticRings(mol),\n",
    "            Descriptors.RingCount(mol),\n",
    "            Descriptors.NumHeteroatoms(mol),\n",
    "            Descriptors.BertzCT(mol)\n",
    "        ]\n",
    "    except:\n",
    "        return [np.nan] * 10\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    \"\"\"ëŒ€íšŒ í‰ê°€ ì ìˆ˜\"\"\"\n",
    "    try:\n",
    "        ic50_true = 10 ** (y_true + 6)\n",
    "        ic50_pred = 10 ** (y_pred + 6)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(ic50_true, ic50_pred))\n",
    "        A = min(rmse / np.mean(ic50_true), 1)\n",
    "        \n",
    "        pic50_true = -y_true\n",
    "        pic50_pred = -y_pred\n",
    "        correlation, _ = pearsonr(pic50_true, pic50_pred)\n",
    "        B = correlation ** 2\n",
    "        \n",
    "        score = 0.4 * (1 - A) + 0.6 * B\n",
    "        return score, A, B\n",
    "    except:\n",
    "        return 0.0, 1.0, 0.0\n",
    "\n",
    "def remove_outliers(X, y, method='iqr', factor=1.5):\n",
    "    \"\"\"ì•„ì›ƒë¼ì´ì–´ ì œê±°\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "    else:  # zscore\n",
    "        z_scores = np.abs((y - y.mean()) / y.std())\n",
    "        mask = z_scores < factor\n",
    "    \n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def optimize_ensemble_weights(models, X_val, y_val):\n",
    "    \"\"\"ìƒê´€ê´€ê³„ ìµœì í™”ë¥¼ ìœ„í•œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "    \n",
    "    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(X_val)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions).T  # (n_samples, n_models)\n",
    "    \n",
    "    def objective(weights):\n",
    "        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡\n",
    "        ensemble_pred = np.average(predictions, axis=1, weights=weights)\n",
    "        \n",
    "        # pIC50 ìƒê´€ê´€ê³„ (B ì ìˆ˜)\n",
    "        pic50_true = -y_val\n",
    "        pic50_pred = -ensemble_pred\n",
    "        \n",
    "        try:\n",
    "            correlation, _ = pearsonr(pic50_true, pic50_pred)\n",
    "            return -(correlation ** 2)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜\n",
    "        except:\n",
    "            return -0.0\n",
    "    \n",
    "    # ì œì•½ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1) for _ in range(len(models))]\n",
    "    \n",
    "    # ì´ˆê¸°ê°’: ê· ë“± ê°€ì¤‘ì¹˜\n",
    "    initial_weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP', \n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result.x if result.success else initial_weights\n",
    "\n",
    "def create_polynomial_features(X, degree=2, interaction_only=True):\n",
    "    \"\"\"ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„± (ì„ íƒì )\"\"\"\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, \n",
    "                             include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    return X_poly, poly\n",
    "\n",
    "# ë©”ì¸ ì½”ë“œ\n",
    "print(\"ğŸš€ ë¹ ë¥¸ ìƒê´€ê´€ê³„ ê°œì„  ì‹œì‘!\")\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "df_train = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/merged_pubchem_chembl.csv\")\n",
    "df_test = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/test.csv\")\n",
    "\n",
    "# SMILES ì»¬ëŸ¼ ì°¾ê¸°\n",
    "smiles_col = None\n",
    "for col in df_train.columns:\n",
    "    if 'smiles' in col.lower():\n",
    "        smiles_col = col\n",
    "        break\n",
    "\n",
    "# 2. ê¸°ìˆ ì ê³„ì‚°\n",
    "print(\"ğŸ§ª ë¶„ì ê¸°ìˆ ì ê³„ì‚°...\")\n",
    "descriptor_results = []\n",
    "for smiles in df_train[smiles_col]:\n",
    "    result = compute_safe_descriptors(smiles)\n",
    "    descriptor_results.append(result)\n",
    "\n",
    "descriptor_df = pd.DataFrame(descriptor_results, columns=[\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT'\n",
    "])\n",
    "\n",
    "# ì¶œì²˜ ì •ë³´\n",
    "if 'source' in df_train.columns:\n",
    "    df_train = pd.get_dummies(df_train, columns=[\"source\"])\n",
    "else:\n",
    "    df_train['source_chembl'] = 1\n",
    "    df_train['source_pubchem'] = 0\n",
    "\n",
    "# 3. ë°ì´í„° ê²°í•©\n",
    "df_combined = pd.concat([df_train.reset_index(drop=True), descriptor_df], axis=1)\n",
    "\n",
    "# 4. íŠ¹ì„± ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„\n",
    "features = [\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT',\n",
    "    'source_chembl', 'source_pubchem'\n",
    "]\n",
    "\n",
    "X = df_combined[features]\n",
    "y = df_combined['log_IC50']\n",
    "\n",
    "# NaN ì œê±°\n",
    "mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "X_clean = X[mask]\n",
    "y_clean = y[mask]\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {len(X_clean)}\")\n",
    "\n",
    "# 5. ì•„ì›ƒë¼ì´ì–´ ì œê±°\n",
    "print(\"ğŸ§¹ ì•„ì›ƒë¼ì´ì–´ ì œê±°...\")\n",
    "X_no_outliers, y_no_outliers = remove_outliers(X_clean, y_clean, method='iqr', factor=2.0)\n",
    "print(f\"ì•„ì›ƒë¼ì´ì–´ ì œê±°: {len(X_clean)} â†’ {len(X_no_outliers)}\")\n",
    "\n",
    "# 6. ë°ì´í„° ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_no_outliers, y_no_outliers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. ê³ ê¸‰ ìŠ¤ì¼€ì¼ë§\n",
    "print(\"ğŸ“Š ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬...\")\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 8. íƒ€ê²Ÿ ë³€í™˜ (ì„ íƒì )\n",
    "print(\"ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ ìµœì í™”...\")\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_val_transformed = target_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 9. ë‹¤ì–‘í•œ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"ğŸ¤– ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Random Forest (ìƒê´€ê´€ê³„ ìµœì í™”)\n",
    "models['rf'] = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=25,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# XGBoost (ìƒê´€ê´€ê³„ íŠ¹í™”)\n",
    "models['xgb'] = xgb.XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "models['lgb'] = lgb.LGBMRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=100,\n",
    "    min_child_samples=10,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Gradient Boosting\n",
    "models['gb'] = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "models['et'] = ExtraTreesRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 10. ëª¨ë¸ í›ˆë ¨ (ë‘ ê°€ì§€ íƒ€ê²Ÿìœ¼ë¡œ)\n",
    "print(\"ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "\n",
    "trained_models_original = {}\n",
    "trained_models_transformed = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  {name} í›ˆë ¨ ì¤‘...\")\n",
    "    \n",
    "    # ì›ë³¸ íƒ€ê²Ÿìœ¼ë¡œ í›ˆë ¨\n",
    "    model_orig = type(model)(**model.get_params())\n",
    "    model_orig.fit(X_train_scaled, y_train)\n",
    "    trained_models_original[name] = model_orig\n",
    "    \n",
    "    # ë³€í™˜ëœ íƒ€ê²Ÿìœ¼ë¡œ í›ˆë ¨\n",
    "    model_trans = type(model)(**model.get_params())\n",
    "    model_trans.fit(X_train_scaled, y_train_transformed)\n",
    "    trained_models_transformed[name] = model_trans\n",
    "\n",
    "# 11. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "print(\"\\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "\n",
    "model_scores = {}\n",
    "model_predictions = {}\n",
    "\n",
    "for name in models.keys():\n",
    "    # ì›ë³¸ íƒ€ê²Ÿ ëª¨ë¸\n",
    "    pred_orig = trained_models_original[name].predict(X_val_scaled)\n",
    "    score_orig, A_orig, B_orig = competition_score(y_val, pred_orig)\n",
    "    \n",
    "    # ë³€í™˜ëœ íƒ€ê²Ÿ ëª¨ë¸ (ì—­ë³€í™˜)\n",
    "    pred_trans_raw = trained_models_transformed[name].predict(X_val_scaled)\n",
    "    pred_trans = target_transformer.inverse_transform(pred_trans_raw.reshape(-1, 1)).ravel()\n",
    "    score_trans, A_trans, B_trans = competition_score(y_val, pred_trans)\n",
    "    \n",
    "    # ë” ì¢‹ì€ ì ìˆ˜ ì„ íƒ\n",
    "    if score_trans > score_orig:\n",
    "        model_scores[name] = score_trans\n",
    "        model_predictions[name] = pred_trans\n",
    "        print(f\"  {name} (ë³€í™˜): {score_trans:.4f} (A={A_trans:.3f}, B={B_trans:.3f})\")\n",
    "    else:\n",
    "        model_scores[name] = score_orig\n",
    "        model_predictions[name] = pred_orig\n",
    "        print(f\"  {name} (ì›ë³¸): {score_orig:.4f} (A={A_orig:.3f}, B={B_orig:.3f})\")\n",
    "\n",
    "# 12. ìƒê´€ê´€ê³„ ìµœì í™” ì•™ìƒë¸”\n",
    "print(\"\\nğŸ­ ìƒê´€ê´€ê³„ ìµœì í™” ì•™ìƒë¸”...\")\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬)\n",
    "sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_models = [name for name, score in sorted_models[:4]]  # ìƒìœ„ 4ê°œ\n",
    "\n",
    "print(f\"ì„ íƒëœ ëª¨ë¸: {top_models}\")\n",
    "\n",
    "# ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "selected_models = []\n",
    "for name in top_models:\n",
    "    if model_scores[name] > model_scores[name.replace('_transformed', '_original')]:\n",
    "        selected_models.append(trained_models_transformed[name])\n",
    "    else:\n",
    "        selected_models.append(trained_models_original[name])\n",
    "\n",
    "optimal_weights = optimize_ensemble_weights(selected_models, X_val_scaled, y_val)\n",
    "print(f\"ìµœì  ê°€ì¤‘ì¹˜: {dict(zip(top_models, optimal_weights))}\")\n",
    "\n",
    "# 13. ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "ensemble_pred = np.zeros(len(X_val_scaled))\n",
    "for i, (name, weight) in enumerate(zip(top_models, optimal_weights)):\n",
    "    pred = model_predictions[name]\n",
    "    ensemble_pred += weight * pred\n",
    "\n",
    "# 14. ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "final_score, final_A, final_B = competition_score(y_val, ensemble_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "\n",
    "print(f\"\\nğŸ‰ ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥:\")\n",
    "print(f\"   Competition Score: {final_score:.4f}\")\n",
    "print(f\"   RMSE: {final_rmse:.4f}\")\n",
    "print(f\"   A (Normalized RMSE): {final_A:.4f}\")\n",
    "print(f\"   B (CorrelationÂ²): {final_B:.4f}\")\n",
    "\n",
    "# ìƒê´€ê´€ê³„ ì„¸ë¶€ ë¶„ì„\n",
    "pic50_true = -y_val\n",
    "pic50_pred = -ensemble_pred\n",
    "correlation, p_value = pearsonr(pic50_true, pic50_pred)\n",
    "print(f\"   pIC50 ìƒê´€ê´€ê³„: {correlation:.4f} (p={p_value:.3e})\")\n",
    "\n",
    "# ë¹ ë¥¸ ìˆ˜ì •: ê¸°ì¡´ ì½”ë“œì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ë¶„ë§Œ êµì²´\n",
    "\n",
    "# 15. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ìˆ˜ì •ëœ ë²„ì „)\n",
    "print(\"\\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ìˆ˜ì •)...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ìˆ ì ê³„ì‚°\n",
    "test_descriptors = []\n",
    "for smiles in df_test[smiles_col]:\n",
    "    result = compute_safe_descriptors(smiles)\n",
    "    test_descriptors.append(result)\n",
    "\n",
    "test_desc_df = pd.DataFrame(test_descriptors, columns=[\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT'\n",
    "])\n",
    "\n",
    "# ì¶œì²˜ ì •ë³´ ì¶”ê°€\n",
    "test_desc_df['source_chembl'] = 0\n",
    "test_desc_df['source_pubchem'] = 1\n",
    "\n",
    "# í›ˆë ¨ì— ì‚¬ìš©ëœ íŠ¹ì„±ê³¼ ì •í™•íˆ ê°™ì€ ìˆœì„œë¡œ ë°ì´í„° ì¤€ë¹„\n",
    "features_ordered = X_train.columns.tolist()  # í›ˆë ¨ ë°ì´í„°ì˜ ì»¬ëŸ¼ ìˆœì„œ\n",
    "print(f\"íŠ¹ì„± ìˆœì„œ: {features_ordered}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°™ì€ ìˆœì„œë¡œ ì •ë ¬\n",
    "X_test_ordered = test_desc_df[features_ordered].copy()\n",
    "\n",
    "# ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì„ íƒ\n",
    "test_mask = ~X_test_ordered.isnull().any(axis=1)\n",
    "X_test_clean = X_test_ordered[test_mask]\n",
    "test_ids = df_test[test_mask]['ID']\n",
    "\n",
    "print(f\"ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test_clean)}/{len(df_test)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: {X_test_clean.shape}\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° í˜•íƒœ: {X_train.shape}\")\n",
    "\n",
    "# ì»¬ëŸ¼ ìˆœì„œ ì¬í™•ì¸\n",
    "if list(X_test_clean.columns) == list(X_train.columns):\n",
    "    print(\"âœ… íŠ¹ì„± ìˆœì„œ ì¼ì¹˜\")\n",
    "else:\n",
    "    print(\"âŒ íŠ¹ì„± ìˆœì„œ ë¶ˆì¼ì¹˜ - ê°•ì œ ì •ë ¬\")\n",
    "    X_test_clean = X_test_clean[X_train.columns]\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "X_test_scaled = scaler.transform(X_test_clean)\n",
    "\n",
    "# ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "test_ensemble_pred = np.zeros(len(X_test_scaled))\n",
    "\n",
    "print(\"ğŸ”® ëª¨ë¸ë³„ ì˜ˆì¸¡ ì¤‘...\")\n",
    "for i, (name, weight) in enumerate(zip(top_models, optimal_weights)):\n",
    "    print(f\"  {name}: ê°€ì¤‘ì¹˜ {weight:.4f}\")\n",
    "    \n",
    "    if model_scores[name] == max([model_scores[n] for n in [name] if n in model_scores]):\n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ ë¡œì§ ìˆ˜ì •\n",
    "        try:\n",
    "            # ë³€í™˜ëœ ëª¨ë¸ì´ ë” ì¢‹ì€ ê²½ìš°\n",
    "            pred_raw = trained_models_transformed[name].predict(X_test_scaled)\n",
    "            pred = target_transformer.inverse_transform(pred_raw.reshape(-1, 1)).ravel()\n",
    "            print(f\"    {name}: ë³€í™˜ ëª¨ë¸ ì‚¬ìš©\")\n",
    "        except:\n",
    "            # ì›ë³¸ ëª¨ë¸ ì‚¬ìš©\n",
    "            pred = trained_models_original[name].predict(X_test_scaled)\n",
    "            print(f\"    {name}: ì›ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
    "    else:\n",
    "        # ì›ë³¸ ëª¨ë¸ ì‚¬ìš©\n",
    "        pred = trained_models_original[name].predict(X_test_scaled)\n",
    "        print(f\"    {name}: ì›ë³¸ ëª¨ë¸ ì‚¬ìš©\")\n",
    "    \n",
    "    test_ensemble_pred += weight * pred\n",
    "\n",
    "# IC50 ë³€í™˜\n",
    "ic50_pred_nM = 10 ** (test_ensemble_pred + 6)\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ ë²”ìœ„ í™•ì¸\n",
    "print(f\"\\nğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
    "print(f\"  log_IC50 ë²”ìœ„: {test_ensemble_pred.min():.3f} ~ {test_ensemble_pred.max():.3f}\")\n",
    "print(f\"  IC50 (nM) ë²”ìœ„: {ic50_pred_nM.min():.1f} ~ {ic50_pred_nM.max():.1f}\")\n",
    "print(f\"  IC50 (nM) ì¤‘ê°„ê°’: {np.median(ic50_pred_nM):.1f}\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'ASK1_IC50_nM': ic50_pred_nM\n",
    "})\n",
    "\n",
    "submission.to_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/fixed_correlation_submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ… fixed_correlation_submission.csv ìƒì„±!\")\n",
    "print(f\"ğŸ“Š ì˜ˆì¸¡ê°’ ìˆ˜: {len(submission)}\")\n",
    "\n",
    "# ì„±ëŠ¥ ìš”ì•½\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ† ìµœì¢… ì„±ëŠ¥ ìš”ì•½\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ê²€ì¦ Competition Score: {final_score:.4f}\")\n",
    "print(f\"A (Normalized RMSE): {final_A:.4f}\")\n",
    "print(f\"B (CorrelationÂ²): {final_B:.4f}\")\n",
    "print(f\"pIC50 ìƒê´€ê´€ê³„: {correlation:.4f}\")\n",
    "print(f\"ì„±ëŠ¥ ê°œì„ : {final_score - 0.4:.4f} (ê¸°ì¤€ 0.4 ëŒ€ë¹„)\")\n",
    "\n",
    "# ëª¨ë¸ ê¸°ì—¬ë„\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ê¸°ì—¬ë„:\")\n",
    "for name, weight in zip(top_models, optimal_weights):\n",
    "    print(f\"  {name}: {weight:.1%}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… ì‘ì—… ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f7620-0cfe-4eb8-acfe-c5aa682c7715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c8e936a-86af-4a68-8665-55bf26c7c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ensemble_stable.csv ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c6fb2-538f-46b8-95a9-fbe26aff0870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8770872f-afde-4ce7-b272-0455e0369fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MAP3K5(ASK1) IC50 ì˜ˆì¸¡ - ê³ ê¸‰ ì•™ìƒë¸” ìµœì í™”\n",
    "- Multi-model Stacking with Optuna\n",
    "- Advanced Feature Engineering\n",
    "- Quantile Matching & Blending\n",
    "\"\"\"\n",
    "\n",
    "# ======================== 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ========================\n",
    "# !pip install optuna lightgbm xgboost catboost rdkit-pypi -q\n",
    "\n",
    "# ======================== 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import rankdata, pearsonr\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, Lipinski, Crippen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit ê²½ê³  ë©”ì‹œì§€ ì™„ì „ ì œê±°\n",
    "import os\n",
    "os.environ['RDK_ERROR_STREAM'] = '/dev/null'\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Optuna ë¡œê¹… ë ˆë²¨ ì„¤ì •\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ======================== 3. ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ========================\n",
    "\n",
    "def calculate_advanced_features(smiles):\n",
    "    \"\"\"í™•ì¥ëœ ë¶„ì ê¸°ìˆ ì ê³„ì‚° - ì•ˆì •í™” ë²„ì „\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # ê¸°ë³¸ ê¸°ìˆ ì (ì•ˆì •ì )\n",
    "        try:\n",
    "            features['MolWt'] = Descriptors.MolWt(mol)\n",
    "            features['LogP'] = Descriptors.MolLogP(mol)\n",
    "            features['TPSA'] = Descriptors.TPSA(mol)\n",
    "            features['NumRotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "            features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n",
    "            features['NumHDonors'] = Descriptors.NumHDonors(mol)\n",
    "            features['NumAromaticRings'] = Descriptors.NumAromaticRings(mol)\n",
    "            features['RingCount'] = Descriptors.RingCount(mol)\n",
    "            features['NumHeteroatoms'] = Descriptors.NumHeteroatoms(mol)\n",
    "            features['HeavyAtomCount'] = Descriptors.HeavyAtomCount(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ê³ ê¸‰ ê¸°ìˆ ì (ë²„ì „ë³„ í˜¸í™˜ì„± ì²´í¬)\n",
    "        try:\n",
    "            features['BertzCT'] = Descriptors.BertzCT(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['Chi0'] = Descriptors.Chi0(mol)\n",
    "            features['Chi1'] = Descriptors.Chi1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['Kappa1'] = Descriptors.Kappa1(mol)\n",
    "            features['Kappa2'] = Descriptors.Kappa2(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['FractionCsp3'] = Descriptors.FractionCsp3(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumSaturatedRings'] = Descriptors.NumSaturatedRings(mol)\n",
    "            features['NumAliphaticRings'] = Descriptors.NumAliphaticRings(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['MolMR'] = Crippen.MolMR(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['BalabanJ'] = Descriptors.BalabanJ(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # VSA ê¸°ìˆ ìë“¤\n",
    "        try:\n",
    "            features['PEOE_VSA1'] = Descriptors.PEOE_VSA1(mol)\n",
    "            features['PEOE_VSA2'] = Descriptors.PEOE_VSA2(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['SMR_VSA1'] = Descriptors.SMR_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['SlogP_VSA1'] = Descriptors.SlogP_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['EState_VSA1'] = Descriptors.EState_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # ì•½ë¬¼ì„± ì§€í‘œ\n",
    "        try:\n",
    "            features['QED'] = Descriptors.qed(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumHeavyAtoms'] = Lipinski.NumHeavyAtoms(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumAliphaticCarbocycles'] = Lipinski.NumAliphaticCarbocycles(mol)\n",
    "            features['NumAliphaticHeterocycles'] = Lipinski.NumAliphaticHeterocycles(mol)\n",
    "            features['NumAromaticCarbocycles'] = Lipinski.NumAromaticCarbocycles(mol)\n",
    "            features['NumAromaticHeterocycles'] = Lipinski.NumAromaticHeterocycles(mol)\n",
    "            features['NumSaturatedCarbocycles'] = Lipinski.NumSaturatedCarbocycles(mol)\n",
    "            features['NumSaturatedHeterocycles'] = Lipinski.NumSaturatedHeterocycles(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # ì¶”ê°€ ì•ˆì •ì ì¸ ê¸°ìˆ ìë“¤\n",
    "        try:\n",
    "            features['NumRadicalElectrons'] = Descriptors.NumRadicalElectrons(mol)\n",
    "            features['NumValenceElectrons'] = Descriptors.NumValenceElectrons(mol)\n",
    "        except: pass\n",
    "        \n",
    "        return features if features else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def get_morgan_fingerprint_features(smiles, radius=2, n_bits=1024):\n",
    "    \"\"\"Morgan Fingerprintë¥¼ í”¼ì²˜ë¡œ ë³€í™˜ - ìƒˆë¡œìš´ API ì‚¬ìš©\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)\n",
    "    \n",
    "    try:\n",
    "        # ìƒˆë¡œìš´ API ì‚¬ìš© (RDKit 2022+)\n",
    "        fp_gen = AllChem.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "        fp = fp_gen.GetFingerprint(mol)\n",
    "        return np.array(fp)\n",
    "    except:\n",
    "        # êµ¬ë²„ì „ API í´ë°±\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "\n",
    "# ======================== 4. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ========================\n",
    "\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df_train = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/chembl_processed_rescaled.csv\")\n",
    "df_test = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/test.csv\")\n",
    "\n",
    "# ë°ì´í„° í´ë¦¬ë‹\n",
    "df_train = df_train[df_train[\"IC50\"] > 0].copy()\n",
    "df_train = df_train[(df_train[\"IC50\"] >= 0.1) & (df_train[\"IC50\"] <= 1e5)].copy()\n",
    "\n",
    "# pIC50 ê³„ì‚°\n",
    "df_train[\"pIC50\"] = 9 - np.log10(df_train[\"IC50\"])\n",
    "\n",
    "# SMILES ì»¬ëŸ¼ ì°¾ê¸°\n",
    "smiles_col = 'Smiles' if 'Smiles' in df_train.columns else 'smiles'\n",
    "smiles_col_test = 'Smiles' if 'Smiles' in df_test.columns else 'smiles'\n",
    "\n",
    "# ê³ ê¸‰ í”¼ì²˜ ì¶”ì¶œ (í•™ìŠµ ë°ì´í„°)\n",
    "print(\"ğŸ§ª ê³ ê¸‰ í”¼ì²˜ ì¶”ì¶œ ì¤‘...\")\n",
    "train_features_list = []\n",
    "for idx, smiles in enumerate(df_train[smiles_col]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"  ì²˜ë¦¬ ì¤‘: {idx}/{len(df_train)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        train_features_list.append(features)\n",
    "    else:\n",
    "        train_features_list.append({})\n",
    "\n",
    "train_features_df = pd.DataFrame(train_features_list)\n",
    "\n",
    "# Morgan Fingerprint ì¶”ê°€ (ì°¨ì› ì¶•ì†Œ)\n",
    "print(\"ğŸ”¬ Morgan Fingerprint ê³„ì‚°...\")\n",
    "n_fp_bits = 256  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì¶•ì†Œ\n",
    "train_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_train[smiles_col]])\n",
    "\n",
    "# PCAë¡œ ì°¨ì› ì¶•ì†Œ\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "train_fp_pca = pca.fit_transform(train_fp_array)\n",
    "train_fp_df = pd.DataFrame(train_fp_pca, columns=[f'FP_PC{i+1}' for i in range(50)])\n",
    "\n",
    "# ëª¨ë“  í”¼ì²˜ ê²°í•©\n",
    "X_full = pd.concat([train_features_df, train_fp_df], axis=1)\n",
    "y_full = df_train[\"pIC50\"]\n",
    "\n",
    "# NaN ì²˜ë¦¬\n",
    "X_full = X_full.fillna(X_full.median())\n",
    "valid_mask = ~(X_full.isnull().any(axis=1) | y_full.isnull())\n",
    "X_clean = X_full[valid_mask]\n",
    "y_clean = y_full[valid_mask]\n",
    "\n",
    "print(f\"âœ… ìœ íš¨ ë°ì´í„°: {len(X_clean)} samples, {X_clean.shape[1]} features\")\n",
    "\n",
    "# ======================== 5. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ë§ ì „ëµ ========================\n",
    "\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'quantile': QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "}\n",
    "\n",
    "X_scaled = {}\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled[name] = scaler.fit_transform(X_clean)\n",
    "    print(f\"  {name} ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ\")\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled['robust'], y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ======================== 6. Optuna ë‹¤ì¤‘ ëª¨ë¸ ìµœì í™” ========================\n",
    "\n",
    "print(\"\\nğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...\")\n",
    "\n",
    "def create_objective(model_type, X_train, y_train, cv_folds=5):\n",
    "    \"\"\"ê° ëª¨ë¸ë³„ Optuna ëª©ì  í•¨ìˆ˜ ìƒì„±\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'lgb':\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'verbosity': -1,\n",
    "                'n_estimators': 500,  # ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            }\n",
    "            model_class = lgb.LGBMRegressor\n",
    "            \n",
    "        elif model_type == 'xgb':\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': 500,  # ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            }\n",
    "            model_class = xgb.XGBRegressor\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': 500,  # ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0.0, 10.0),\n",
    "                'verbose': False,\n",
    "            }\n",
    "            model_class = cb.CatBoostRegressor\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_float('max_features', 0.3, 1.0),\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42,\n",
    "            }\n",
    "            model_class = RandomForestRegressor\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        rmse_list = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_fold_train = X_train[train_idx]\n",
    "            X_fold_val = X_train[val_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]\n",
    "            \n",
    "            model = model_class(**params)\n",
    "            \n",
    "            # ê°„ë‹¨í•œ í•™ìŠµ (early stopping ì—†ì´)\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            preds = model.predict(X_fold_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "            rmse_list.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_list)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "# ê° ëª¨ë¸ ìµœì í™” - ì†ë„ë¥¼ ìœ„í•´ trials ìˆ˜ ì¤„ì„\n",
    "best_params = {}\n",
    "studies = {}\n",
    "\n",
    "for model_type in ['lgb', 'xgb', 'catboost', 'rf']:\n",
    "    print(f\"\\n  {model_type.upper()} ìµœì í™” ì¤‘...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(\n",
    "        create_objective(model_type, X_train, y_train),\n",
    "        n_trials=10,  # ì†ë„ë¥¼ ìœ„í•´ ì¤„ì„ (ì‹¤ì œë¡œëŠ” 50-100 ê¶Œì¥)\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    best_params[model_type] = study.best_params\n",
    "    studies[model_type] = study\n",
    "    print(f\"    Best RMSE: {study.best_value:.4f}\")\n",
    "\n",
    "# ======================== 7. ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ ========================\n",
    "\n",
    "print(\"\\nğŸ¤– ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# LightGBM\n",
    "models['lgb'] = lgb.LGBMRegressor(**best_params['lgb'], n_estimators=1000, verbosity=-1)\n",
    "try:\n",
    "    models['lgb'].fit(X_train, y_train, \n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "except:\n",
    "    # í´ë°±: callbacks ì—†ì´\n",
    "    models['lgb'].fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "models['xgb'] = xgb.XGBRegressor(**best_params['xgb'], n_estimators=1000)\n",
    "try:\n",
    "    # ìƒˆ ë²„ì „ (XGBoost 2.0+)\n",
    "    models['xgb'].set_params(early_stopping_rounds=50)\n",
    "    models['xgb'].fit(X_train, y_train,\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      verbose=False)\n",
    "except:\n",
    "    try:\n",
    "        # êµ¬ ë²„ì „\n",
    "        models['xgb'].fit(X_train, y_train,\n",
    "                          eval_set=[(X_val, y_val)],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose=False)\n",
    "    except:\n",
    "        # early stopping ì—†ì´\n",
    "        models['xgb'].fit(X_train, y_train)\n",
    "\n",
    "# CatBoost\n",
    "models['catboost'] = cb.CatBoostRegressor(**best_params['catboost'], iterations=1000)\n",
    "try:\n",
    "    models['catboost'].fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False, early_stopping_rounds=50)\n",
    "except:\n",
    "    models['catboost'].fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Random Forest\n",
    "models['rf'] = RandomForestRegressor(**best_params['rf'])\n",
    "models['rf'].fit(X_train, y_train)\n",
    "\n",
    "# Extra Trees (ê³ ì • íŒŒë¼ë¯¸í„°)\n",
    "models['extra'] = ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1)\n",
    "models['extra'].fit(X_train, y_train)\n",
    "\n",
    "# Neural Network\n",
    "models['mlp'] = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "models['mlp'].fit(X_train, y_train)\n",
    "\n",
    "# ======================== 8. ëª¨ë¸ í‰ê°€ ë° ê°€ì¤‘ì¹˜ ìµœì í™” ========================\n",
    "\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...\")\n",
    "\n",
    "val_predictions = {}\n",
    "val_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    \n",
    "    # pIC50 ìƒê´€ê´€ê³„\n",
    "    corr, _ = pearsonr(y_val, pred)\n",
    "    \n",
    "    val_predictions[name] = pred\n",
    "    val_scores[name] = {'rmse': rmse, 'r2': r2, 'corr': corr}\n",
    "    \n",
    "    print(f\"  {name:10s}: RMSE={rmse:.4f}, RÂ²={r2:.4f}, Corr={corr:.4f}\")\n",
    "\n",
    "# ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def ensemble_objective(weights):\n",
    "    ensemble_pred = np.zeros(len(y_val))\n",
    "    for i, name in enumerate(models.keys()):\n",
    "        ensemble_pred += weights[i] * val_predictions[name]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "    return rmse\n",
    "\n",
    "# ì œì•½ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1) for _ in range(len(models))]\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "result = minimize(ensemble_objective, initial_weights, \n",
    "                 method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "optimal_weights = result.x\n",
    "print(f\"\\nâœ… ìµœì  ê°€ì¤‘ì¹˜:\")\n",
    "for name, weight in zip(models.keys(), optimal_weights):\n",
    "    if weight > 0.01:\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "\n",
    "# ======================== 9. ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ========================\n",
    "\n",
    "print(\"\\nğŸ”„ ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ...\")\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "X_full_scaled = scalers['robust'].fit_transform(X_clean)\n",
    "\n",
    "models_full = {}\n",
    "\n",
    "# ê° ëª¨ë¸ì„ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\n",
    "for name in models.keys():\n",
    "    if name == 'lgb':\n",
    "        models_full[name] = lgb.LGBMRegressor(**best_params['lgb'], n_estimators=1200, verbosity=-1)\n",
    "    elif name == 'xgb':\n",
    "        models_full[name] = xgb.XGBRegressor(**best_params['xgb'], n_estimators=1200)\n",
    "    elif name == 'catboost':\n",
    "        models_full[name] = cb.CatBoostRegressor(**best_params['catboost'], iterations=1200, verbose=False)\n",
    "    elif name == 'rf':\n",
    "        models_full[name] = RandomForestRegressor(**best_params['rf'])\n",
    "    elif name == 'extra':\n",
    "        models_full[name] = ExtraTreesRegressor(n_estimators=600, max_depth=20, random_state=42, n_jobs=-1)\n",
    "    elif name == 'mlp':\n",
    "        models_full[name] = MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128, 64),\n",
    "            activation='relu',\n",
    "            max_iter=1500,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    models_full[name].fit(X_full_scaled, y_clean)\n",
    "    print(f\"  {name} í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "# ======================== 10. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ========================\n",
    "\n",
    "print(\"\\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í”¼ì²˜ ì¶”ì¶œ\n",
    "test_features_list = []\n",
    "for idx, smiles in enumerate(df_test[smiles_col_test]):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"  ì²˜ë¦¬ ì¤‘: {idx}/{len(df_test)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        test_features_list.append(features)\n",
    "    else:\n",
    "        test_features_list.append({})\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features_list)\n",
    "\n",
    "# Morgan Fingerprint\n",
    "test_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_test[smiles_col_test]])\n",
    "test_fp_pca = pca.transform(test_fp_array)\n",
    "test_fp_df = pd.DataFrame(test_fp_pca, columns=[f'FP_PC{i+1}' for i in range(50)])\n",
    "\n",
    "# ê²°í•©\n",
    "X_test_full = pd.concat([test_features_df, test_fp_df], axis=1)\n",
    "X_test_full = X_test_full.fillna(X_test_full.median())\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥\n",
    "X_test_full = X_test_full[X_clean.columns]\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "X_test_scaled = scalers['robust'].transform(X_test_full)\n",
    "\n",
    "# ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "test_predictions = {}\n",
    "for name, model in models_full.items():\n",
    "    test_predictions[name] = model.predict(X_test_scaled)\n",
    "    print(f\"  {name} ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "\n",
    "# ======================== 11. Quantile Matching & ì•™ìƒë¸” ========================\n",
    "\n",
    "print(\"\\nğŸ­ Quantile Matching & ìµœì¢… ì•™ìƒë¸”...\")\n",
    "\n",
    "def quantile_match(source_pred, target_pred):\n",
    "    \"\"\"Quantile Matchingìœ¼ë¡œ ë¶„í¬ ì •ë ¬\"\"\"\n",
    "    sorted_target = np.sort(target_pred)\n",
    "    source_ranks = rankdata(source_pred, method='ordinal') - 1\n",
    "    source_ranks = np.clip(source_ranks, 0, len(sorted_target)-1).astype(int)\n",
    "    return sorted_target[source_ranks]\n",
    "\n",
    "# RFë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ Quantile Matching\n",
    "base_pred = test_predictions['rf']\n",
    "matched_predictions = {'rf': base_pred}\n",
    "\n",
    "for name in ['lgb', 'xgb', 'catboost', 'extra', 'mlp']:\n",
    "    matched_predictions[name] = quantile_match(test_predictions[name], base_pred)\n",
    "\n",
    "# ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”\n",
    "ensemble_pred = np.zeros(len(X_test_scaled))\n",
    "for i, name in enumerate(models.keys()):\n",
    "    ensemble_pred += optimal_weights[i] * matched_predictions[name]\n",
    "\n",
    "# ======================== 12. í›„ì²˜ë¦¬ ë° ì œì¶œ íŒŒì¼ ìƒì„± ========================\n",
    "\n",
    "# í´ë¦¬í•‘\n",
    "ensemble_pred = np.clip(ensemble_pred, y_clean.min(), y_clean.max())\n",
    "\n",
    "# IC50 ì—­ë³€í™˜\n",
    "ic50_pred = 10 ** (9 - ensemble_pred)\n",
    "\n",
    "# ì¶”ê°€ í›„ì²˜ë¦¬: ê·¹ë‹¨ê°’ ì œí•œ\n",
    "ic50_pred = np.clip(ic50_pred, 0.1, 100000)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": ic50_pred\n",
    "})\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "output_dir = \"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/submissions/\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ì €ì¥\n",
    "submission.to_csv(output_dir + \"submit_advanced_ensemble.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸŠ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ì˜ˆì¸¡ í†µê³„:\")\n",
    "print(f\"  IC50 ë²”ìœ„: {ic50_pred.min():.2f} ~ {ic50_pred.max():.2f} nM\")\n",
    "print(f\"  IC50 ì¤‘ê°„ê°’: {np.median(ic50_pred):.2f} nM\")\n",
    "print(f\"  IC50 í‰ê· : {np.mean(ic50_pred):.2f} nM\")\n",
    "print(f\"  IC50 í‘œì¤€í¸ì°¨: {np.std(ic50_pred):.2f} nM\")\n",
    "print(\"\\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥: submit_advanced_ensemble.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ======================== 13. ì¶”ê°€: Stacking ì•™ìƒë¸” (ì„ íƒì‚¬í•­) ========================\n",
    "\n",
    "print(\"\\nğŸ”¥ ë³´ë„ˆìŠ¤: Stacking ì•™ìƒë¸”...\")\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ë“¤\n",
    "base_models = [\n",
    "    ('lgb', lgb.LGBMRegressor(**best_params['lgb'], n_estimators=500, verbosity=-1)),\n",
    "    ('xgb', xgb.XGBRegressor(**best_params['xgb'], n_estimators=500)),\n",
    "    ('rf', RandomForestRegressor(**best_params['rf'])),\n",
    "]\n",
    "\n",
    "# ë©”íƒ€ ëª¨ë¸\n",
    "meta_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "\n",
    "# Stacking\n",
    "stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "stacking.fit(X_full_scaled, y_clean)\n",
    "\n",
    "# Stacking ì˜ˆì¸¡\n",
    "stacking_pred = stacking.predict(X_test_scaled)\n",
    "stacking_pred = np.clip(stacking_pred, y_clean.min(), y_clean.max())\n",
    "ic50_stacking = 10 ** (9 - stacking_pred)\n",
    "ic50_stacking = np.clip(ic50_stacking, 0.1, 100000)\n",
    "\n",
    "# Stacking ì œì¶œ íŒŒì¼\n",
    "submission_stacking = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": ic50_stacking\n",
    "})\n",
    "submission_stacking.to_csv(output_dir + \"submit_stacking.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Stacking ì œì¶œ íŒŒì¼ ì €ì¥: submit_stacking.csv\")\n",
    "\n",
    "# ìµœì¢… ë¸”ë Œë”© (ì•™ìƒë¸” + Stacking)\n",
    "final_pred = 0.7 * ic50_pred + 0.3 * ic50_stacking\n",
    " \n",
    "submission_final = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": final_pred\n",
    "})\n",
    "submission_final.to_csv(output_dir + \"submit_final_blend.csv\", index=False)\n",
    "\n",
    "print(\"âœ… ìµœì¢… ë¸”ë Œë”© ì œì¶œ íŒŒì¼ ì €ì¥: submit_final_blend.csv\")\n",
    "print(\"\\nğŸ† ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22af310d-96a5-4b5f-a103-4b62ceff7591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>IC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL324340</td>\n",
       "      <td>Cc1ccc2oc(-c3cccc(N4C(=O)c5ccc(C(=O)O)cc5C4=O)...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.71</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.374</td>\n",
       "      <td>4.30202</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL109600</td>\n",
       "      <td>COc1ccccc1-c1ccc2oc(-c3ccc(OC)c(N4C(=O)c5ccc(C...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>119.17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>520.497</td>\n",
       "      <td>5.67780</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL357278</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4ccc(Cl)c(C(...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>543.011</td>\n",
       "      <td>4.27292</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL357119</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>468.623</td>\n",
       "      <td>2.32092</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL152968</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>516.667</td>\n",
       "      <td>4.26772</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Smiles  \\\n",
       "0  CHEMBL324340  Cc1ccc2oc(-c3cccc(N4C(=O)c5ccc(C(=O)O)cc5C4=O)...   \n",
       "1  CHEMBL109600  COc1ccccc1-c1ccc2oc(-c3ccc(OC)c(N4C(=O)c5ccc(C...   \n",
       "2  CHEMBL357278  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4ccc(Cl)c(C(...   \n",
       "3  CHEMBL357119  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...   \n",
       "4  CHEMBL152968  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...   \n",
       "\n",
       "   NumHDonors    TPSA  NumHAcceptors    MolWt     LogP  NumRotatableBonds  \\\n",
       "0         1.0  100.71            5.0  398.374  4.30202                3.0   \n",
       "1         1.0  119.17            7.0  520.497  5.67780                6.0   \n",
       "2         2.0   77.93            7.0  543.011  4.27292                8.0   \n",
       "3         2.0   77.93            7.0  468.623  2.32092               10.0   \n",
       "4         2.0   77.93            7.0  516.667  4.26772                9.0   \n",
       "\n",
       "   RingCount  HeavyAtomCount     IC50  \n",
       "0        5.0            30.0   2500.0  \n",
       "1        6.0            39.0   9000.0  \n",
       "2        4.0            36.0   4000.0  \n",
       "3        4.0            33.0  17000.0  \n",
       "4        5.0            37.0    180.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/ChEMBL_IC50_30k_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6ae854-a301-4f51-b653-0d21005d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
