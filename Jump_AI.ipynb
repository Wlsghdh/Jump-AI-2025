{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1ff0df-0541-4e38-895f-f21758da3105",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e51abf7-4d67-40c4-a8ef-1c876df2f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import rankdata, pearsonr\n",
    "from scipy.optimize import minimize\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, Lipinski, Crippen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['RDK_ERROR_STREAM'] = '/dev/null'\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b84f42-19f3-4e84-8509-43774326c234",
   "metadata": {},
   "source": [
    "## feature cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a1e55b-ca46-4155-aa9e-75689e0900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_features(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            features['MolWt'] = Descriptors.MolWt(mol)\n",
    "            features['LogP'] = Descriptors.MolLogP(mol)\n",
    "            features['TPSA'] = Descriptors.TPSA(mol)\n",
    "            features['NumRotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "            features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n",
    "            features['NumHDonors'] = Descriptors.NumHDonors(mol)\n",
    "            features['NumAromaticRings'] = Descriptors.NumAromaticRings(mol)\n",
    "            features['RingCount'] = Descriptors.RingCount(mol)\n",
    "            features['NumHeteroatoms'] = Descriptors.NumHeteroatoms(mol)\n",
    "            features['HeavyAtomCount'] = Descriptors.HeavyAtomCount(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            features['BertzCT'] = Descriptors.BertzCT(mol)\n",
    "            features['Chi0'] = Descriptors.Chi0(mol)\n",
    "            features['Chi1'] = Descriptors.Chi1(mol)\n",
    "            features['HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
    "            features['Kappa1'] = Descriptors.Kappa1(mol)\n",
    "            features['Kappa2'] = Descriptors.Kappa2(mol)\n",
    "            features['FractionCsp3'] = Descriptors.FractionCsp3(mol)\n",
    "            features['NumSaturatedRings'] = Descriptors.NumSaturatedRings(mol)\n",
    "            features['NumAliphaticRings'] = Descriptors.NumAliphaticRings(mol)\n",
    "            features['MolMR'] = Crippen.MolMR(mol)\n",
    "            features['BalabanJ'] = Descriptors.BalabanJ(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            features['PEOE_VSA1'] = Descriptors.PEOE_VSA1(mol)\n",
    "            features['PEOE_VSA2'] = Descriptors.PEOE_VSA2(mol)\n",
    "            features['SMR_VSA1'] = Descriptors.SMR_VSA1(mol)\n",
    "            features['SlogP_VSA1'] = Descriptors.SlogP_VSA1(mol)\n",
    "            features['EState_VSA1'] = Descriptors.EState_VSA1(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            features['QED'] = Descriptors.qed(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            features['NumHeavyAtoms'] = Lipinski.NumHeavyAtoms(mol)\n",
    "            features['NumAliphaticCarbocycles'] = Lipinski.NumAliphaticCarbocycles(mol)\n",
    "            features['NumAliphaticHeterocycles'] = Lipinski.NumAliphaticHeterocycles(mol)\n",
    "            features['NumAromaticCarbocycles'] = Lipinski.NumAromaticCarbocycles(mol)\n",
    "            features['NumAromaticHeterocycles'] = Lipinski.NumAromaticHeterocycles(mol)\n",
    "            features['NumSaturatedCarbocycles'] = Lipinski.NumSaturatedCarbocycles(mol)\n",
    "            features['NumSaturatedHeterocycles'] = Lipinski.NumSaturatedHeterocycles(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            features['NumRadicalElectrons'] = Descriptors.NumRadicalElectrons(mol)\n",
    "            features['NumValenceElectrons'] = Descriptors.NumValenceElectrons(mol)\n",
    "            \n",
    "            features['FlexibilityIndex'] = features.get('NumRotatableBonds', 0) / max(features.get('HeavyAtomCount', 1), 1)\n",
    "            features['TPSARatio'] = features.get('TPSA', 0) / max(features.get('MolWt', 1), 1)\n",
    "            features['AromaticRatio'] = features.get('NumAromaticRings', 0) / max(features.get('RingCount', 1), 1) if features.get('RingCount', 0) > 0 else 0\n",
    "            features['HeteroatomRatio'] = features.get('NumHeteroatoms', 0) / max(features.get('HeavyAtomCount', 1), 1)\n",
    "            \n",
    "            violations = 0\n",
    "            if features.get('MolWt', 0) > 500: violations += 1\n",
    "            if features.get('LogP', 0) > 5: violations += 1\n",
    "            if features.get('NumHDonors', 0) > 5: violations += 1\n",
    "            if features.get('NumHAcceptors', 0) > 10: violations += 1\n",
    "            features['LipinskiViolations'] = violations\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return features if features else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f3257-ad4e-4ef8-a24f-10d9a8e70447",
   "metadata": {},
   "source": [
    "## morgan fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b891af-ca70-4ac7-8024-7dff415e63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morgan_fingerprint_features(smiles, radius=2, n_bits=1024):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)\n",
    "    \n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    except Exception as e:\n",
    "        return np.zeros(n_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6af888-0f9e-431e-b77a-c671f2c7a4bb",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6700a4dc-bec5-4dd1-a2ad-8fe0c30e2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective_v2(model_type, X_train, y_train, cv_folds=5):\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'lgb':\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'verbosity': -1,\n",
    "                'n_estimators': 800,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 400),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 150),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 15.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 15.0),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "            }\n",
    "            model_class = lgb.LGBMRegressor\n",
    "            \n",
    "        elif model_type == 'xgb':\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': 800,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 15.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 15.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 8.0),\n",
    "            }\n",
    "            model_class = xgb.XGBRegressor\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': 200,\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 8),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'verbose': False,\n",
    "                'thread_count': 4,\n",
    "                'random_seed': 42,\n",
    "            }\n",
    "            model_class = cb.CatBoostRegressor\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n",
    "                'max_depth': trial.suggest_int('max_depth', 8, 35),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 8),\n",
    "                'max_features': trial.suggest_float('max_features', 0.4, 1.0),\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42,\n",
    "            }\n",
    "            model_class = RandomForestRegressor\n",
    "        \n",
    "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        rmse_list = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_fold_train = X_train[train_idx]\n",
    "            X_fold_val = X_train[val_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]\n",
    "            \n",
    "            model = model_class(**params)\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            preds = model.predict(X_fold_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "            rmse_list.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_list)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38eade6-244b-470a-b504-0be3f36a1b03",
   "metadata": {},
   "source": [
    "## ensemble blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdd4ddd4-7fc2-44e5-b04c-44793f3d2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_match(source_pred, target_pred):\n",
    "    sorted_target = np.sort(target_pred)\n",
    "    source_ranks = rankdata(source_pred, method='ordinal') - 1\n",
    "    source_ranks = np.clip(source_ranks, 0, len(sorted_target)-1).astype(int)\n",
    "    return sorted_target[source_ranks]\n",
    "\n",
    "def advanced_ensemble_blend(predictions_dict, weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(predictions_dict)) / len(predictions_dict)\n",
    "    \n",
    "    weighted_avg = np.zeros(len(list(predictions_dict.values())[0]))\n",
    "    for i, pred in enumerate(predictions_dict.values()):\n",
    "        weighted_avg += weights[i] * pred\n",
    "    \n",
    "    ranked_preds = {}\n",
    "    for name, pred in predictions_dict.items():\n",
    "        ranked_preds[name] = rankdata(pred) / len(pred)\n",
    "    \n",
    "    avg_ranks = np.zeros(len(list(predictions_dict.values())[0]))\n",
    "    for i, pred in enumerate(ranked_preds.values()):\n",
    "        avg_ranks += weights[i] * pred\n",
    "    \n",
    "    base_pred = list(predictions_dict.values())[0]\n",
    "    sorted_base = np.sort(base_pred)\n",
    "    rank_indices = (avg_ranks * (len(sorted_base) - 1)).astype(int)\n",
    "    rank_indices = np.clip(rank_indices, 0, len(sorted_base) - 1)\n",
    "    rank_avg = sorted_base[rank_indices]\n",
    "    \n",
    "    final_blend = 0.7 * weighted_avg + 0.3 * rank_avg\n",
    "    \n",
    "    return final_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465bd95-2519-4d5b-b77b-9610d5b9508e",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef6dc34a-c8f0-4b28-bee9-529afd3c5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/chembl_processed_rescaled.csv\")\n",
    "df_test = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/test.csv\")\n",
    "\n",
    "df_train = df_train[df_train[\"IC50\"] > 0].copy()\n",
    "df_train = df_train[(df_train[\"IC50\"] >= 0.1) & (df_train[\"IC50\"] <= 1e5)].copy()\n",
    "df_train[\"pIC50\"] = 9 - np.log10(df_train[\"IC50\"])\n",
    "\n",
    "smiles_col = 'Smiles' if 'Smiles' in df_train.columns else 'smiles'\n",
    "smiles_col_test = 'Smiles' if 'Smiles' in df_test.columns else 'smiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a34363-8895-47c0-a661-8ee6c61d4aa8",
   "metadata": {},
   "source": [
    "## feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a56dc5bb-6013-46ad-8a33-76b3faa8d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  처리 중: 0/806\n",
      "  처리 중: 200/806\n",
      "  처리 중: 400/806\n",
      "  처리 중: 600/806\n",
      "  처리 중: 800/806\n",
      "분자 기술자 완료: (806, 29)\n"
     ]
    }
   ],
   "source": [
    "train_features_list = []\n",
    "for idx, smiles in enumerate(df_train[smiles_col]):\n",
    "    if idx % 200 == 0:\n",
    "        print(f\"  처리 중: {idx}/{len(df_train)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        train_features_list.append(features)\n",
    "    else:\n",
    "        train_features_list.append({})\n",
    "\n",
    "train_features_df = pd.DataFrame(train_features_list)\n",
    "print(f\"분자 기술자 완료: {train_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a3594-4dc0-4d7b-bed7-34587aede8d8",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d817424a-6931-4d4f-9b43-bfab9a2d045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morgan Fingerprint 완료: (806, 100)\n"
     ]
    }
   ],
   "source": [
    "n_fp_bits = 1024\n",
    "train_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_train[smiles_col]])\n",
    "\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "train_fp_pca = pca.fit_transform(train_fp_array)\n",
    "train_fp_df = pd.DataFrame(train_fp_pca, columns=[f'FP_PC{i+1}' for i in range(100)])\n",
    "\n",
    "print(f\"Morgan Fingerprint 완료: {train_fp_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8aba7-57b1-435d-9b93-6bec1a498086",
   "metadata": {},
   "source": [
    "## Feature concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0305c3b-4df6-427d-ac25-99e8232527e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([train_features_df, train_fp_df], axis=1)\n",
    "y_full = df_train[\"pIC50\"]\n",
    "\n",
    "X_full = X_full.fillna(X_full.median())\n",
    "valid_mask = ~(X_full.isnull().any(axis=1) | y_full.isnull())\n",
    "X_clean = X_full[valid_mask]\n",
    "y_clean = y_full[valid_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5b0e7-fa2f-4ae5-b458-9cf6e40db23d",
   "metadata": {},
   "source": [
    "## Multi-Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1740537-befe-4616-8eac-5add50352a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습/검증 분할 완료: 644 / 162\n"
     ]
    }
   ],
   "source": [
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'quantile': QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "}\n",
    "\n",
    "X_scaled = {}\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled[name] = scaler.fit_transform(X_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled['robust'], y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"학습/검증 분할 완료: {X_train.shape[0]} / {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde85762-18a3-416d-b35e-57b96d980b7f",
   "metadata": {},
   "source": [
    "## Model clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdd74b48-09ab-4e95-b69d-287047d561dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LGB 최적화\n",
      "    Best RMSE: 0.9162\n",
      "  XGB 최적화\n",
      "    Best RMSE: 0.9173\n",
      "  RF 최적화\n",
      "    Best RMSE: 0.9033\n"
     ]
    }
   ],
   "source": [
    "best_params = {}\n",
    "studies = {}\n",
    "\n",
    "for model_type in ['lgb', 'xgb', 'rf']:\n",
    "    print(f\"  {model_type.upper()} 최적화\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(\n",
    "        create_objective_v2(model_type, X_train, y_train),\n",
    "        n_trials=30,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    best_params[model_type] = study.best_params\n",
    "    studies[model_type] = study\n",
    "    print(f\"    Best RMSE: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99082d88-1288-47b1-a7f9-ca6ed1efa84f",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6062c24-7328-40ac-ac78-8b5076fa702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 모델 학습 완료오\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "models['lgb'] = lgb.LGBMRegressor(**best_params['lgb'], n_estimators=1200, verbosity=-1)\n",
    "try:\n",
    "    models['lgb'].fit(X_train, y_train, \n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      callbacks=[lgb.early_stopping(80, verbose=False)])\n",
    "except:\n",
    "    models['lgb'].fit(X_train, y_train)\n",
    "\n",
    "models['xgb'] = xgb.XGBRegressor(**best_params['xgb'], n_estimators=1200)\n",
    "try:\n",
    "    models['xgb'].set_params(early_stopping_rounds=80)\n",
    "    models['xgb'].fit(X_train, y_train,\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      verbose=False)\n",
    "except:\n",
    "    try:\n",
    "        models['xgb'].fit(X_train, y_train,\n",
    "                          eval_set=[(X_val, y_val)],\n",
    "                          early_stopping_rounds=80,\n",
    "                          verbose=False)\n",
    "    except:\n",
    "        models['xgb'].fit(X_train, y_train)\n",
    "\n",
    "models['rf'] = RandomForestRegressor(**best_params['rf'])\n",
    "models['rf'].fit(X_train, y_train)\n",
    "\n",
    "models['extra'] = ExtraTreesRegressor(n_estimators=600, max_depth=25, random_state=42, n_jobs=-1)\n",
    "models['extra'].fit(X_train, y_train)\n",
    "\n",
    "models['mlp'] = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=1200,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "models['mlp'].fit(X_train, y_train)\n",
    "\n",
    "print(\"모든 모델 학습 완료오\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6569475-0932-4a68-a933-a6692082a95a",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243a8611-f2c1-4cb8-84fb-89129ef545ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lgb       : RMSE=0.8587, R²=0.3449, Corr=0.5888\n",
      "  xgb       : RMSE=0.8653, R²=0.3348, Corr=0.5799\n",
      "  rf        : RMSE=0.8954, R²=0.2877, Corr=0.5475\n",
      "  extra     : RMSE=1.0330, R²=0.0519, Corr=0.4537\n",
      "  mlp       : RMSE=1.4678, R²=-0.9141, Corr=0.3368\n"
     ]
    }
   ],
   "source": [
    "val_predictions = {}\n",
    "val_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    corr, _ = pearsonr(y_val, pred)\n",
    "    \n",
    "    val_predictions[name] = pred\n",
    "    val_scores[name] = {'rmse': rmse, 'r2': r2, 'corr': corr}\n",
    "    \n",
    "    print(f\"  {name:10s}: RMSE={rmse:.4f}, R²={r2:.4f}, Corr={corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17ae95-8b40-4fce-9e6a-21965bd4ec1d",
   "metadata": {},
   "source": [
    "## ensemble weighted clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2c20377-b051-402a-b05e-28b1e28b9c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 가중치:\n",
      "  lgb: 0.725\n",
      "  xgb: 0.275\n"
     ]
    }
   ],
   "source": [
    "def ensemble_objective(weights):\n",
    "    ensemble_pred = np.zeros(len(y_val))\n",
    "    for i, name in enumerate(models.keys()):\n",
    "        ensemble_pred += weights[i] * val_predictions[name]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "    return rmse\n",
    "\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1) for _ in range(len(models))]\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "result = minimize(ensemble_objective, initial_weights, \n",
    "                 method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "optimal_weights = result.x\n",
    "print(f\"최적 가중치:\")\n",
    "for name, weight in zip(models.keys(), optimal_weights):\n",
    "    if weight > 0.01:\n",
    "        print(f\"  {name}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff1379-ada4-4924-8221-fa29a88116b7",
   "metadata": {},
   "source": [
    "## retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e914ec02-48ef-4d91-b709-1251d5512b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_full = {}\n",
    "\n",
    "for name in models.keys():\n",
    "    if name == 'lgb':\n",
    "        lgb_params = {k: v for k, v in best_params['lgb'].items()}\n",
    "        models_full[name] = lgb.LGBMRegressor(**lgb_params, n_estimators=1500, verbosity=-1)\n",
    "    elif name == 'xgb':\n",
    "        xgb_params = {k: v for k, v in best_params['xgb'].items()}\n",
    "        models_full[name] = xgb.XGBRegressor(**xgb_params, n_estimators=1500)\n",
    "    elif name == 'rf':\n",
    "        rf_params = {k: v for k, v in best_params['rf'].items()}\n",
    "        models_full[name] = RandomForestRegressor(**rf_params)\n",
    "    elif name == 'extra':\n",
    "        models_full[name] = ExtraTreesRegressor(n_estimators=800, max_depth=30, random_state=42, n_jobs=-1)\n",
    "    elif name == 'mlp':\n",
    "        models_full[name] = MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128, 64),\n",
    "            activation='relu',\n",
    "            max_iter=1500,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    models_full[name].fit(X_scaled['robust'], y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419bba89-eaad-4ce9-aacb-25689242b540",
   "metadata": {},
   "source": [
    "## Test Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e3971d3-431e-49aa-9ea1-182b1222b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  처리 중: 0/127\n",
      "  처리 중: 50/127\n",
      "  처리 중: 100/127\n"
     ]
    }
   ],
   "source": [
    "test_features_list = []\n",
    "for idx, smiles in enumerate(df_test[smiles_col_test]):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"  처리 중: {idx}/{len(df_test)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        test_features_list.append(features)\n",
    "    else:\n",
    "        test_features_list.append({})\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features_list)\n",
    "\n",
    "test_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_test[smiles_col_test]])\n",
    "test_fp_pca = pca.transform(test_fp_array)\n",
    "test_fp_df = pd.DataFrame(test_fp_pca, columns=[f'FP_PC{i+1}' for i in range(100)])\n",
    "\n",
    "X_test_full = pd.concat([test_features_df, test_fp_df], axis=1)\n",
    "X_test_full = X_test_full.fillna(X_test_full.median())\n",
    "\n",
    "missing_cols = set(X_clean.columns) - set(X_test_full.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_full[col] = 0\n",
    "\n",
    "X_test_full = X_test_full[X_clean.columns]\n",
    "X_test_scaled = scalers['robust'].transform(X_test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6a2ee-df2e-4f54-8ad4-f1a3417e8b7f",
   "metadata": {},
   "source": [
    "## Predicted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d672790-217b-4588-9ffc-3d6e1d4194bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lgb 예측 완료\n",
      "  xgb 예측 완료\n",
      "  rf 예측 완료\n",
      "  extra 예측 완료\n",
      "  mlp 예측 완료\n"
     ]
    }
   ],
   "source": [
    "test_predictions = {}\n",
    "for name, model in models_full.items():\n",
    "    test_predictions[name] = model.predict(X_test_scaled)\n",
    "    print(f\"  {name} 예측 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca525933-5f74-4af4-978a-0e8cca3b2051",
   "metadata": {},
   "source": [
    "## ensemble blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c82c799d-8818-462a-be6c-c8b1b5f9e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = advanced_ensemble_blend(test_predictions, optimal_weights)\n",
    "\n",
    "base_pred = test_predictions['rf']\n",
    "matched_predictions = {}\n",
    "\n",
    "for name in test_predictions.keys():\n",
    "    matched_predictions[name] = quantile_match(test_predictions[name], base_pred)\n",
    "\n",
    "matched_ensemble = advanced_ensemble_blend(matched_predictions, optimal_weights)\n",
    "\n",
    "final_pred = 0.6 * ensemble_pred + 0.4 * matched_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052b790-da82-4812-a2f2-8aa7fe78f7ce",
   "metadata": {},
   "source": [
    "## submit submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9454876-cdb0-4f83-92de-583660243e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  submit_conservative_weighted_only.csv 저장 완료\n",
      "  submit_conservative_quantile_matched.csv 저장 완료\n",
      "  submit_conservative_final_meta.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "final_pred = np.clip(final_pred, y_clean.min(), y_clean.max())\n",
    "ic50_pred = 10 ** (9 - final_pred)\n",
    "ic50_pred = np.clip(ic50_pred, 0.1, 100000)\n",
    "\n",
    "output_dir = \"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/submissions/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": ic50_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(output_dir + \"submit_conservative_enhanced.csv\", index=False)\n",
    "\n",
    "ensemble_strategies = {\n",
    "    'weighted_only': ensemble_pred,\n",
    "    'quantile_matched': matched_ensemble,\n",
    "    'final_meta': final_pred\n",
    "}\n",
    "\n",
    "for strategy_name, pred in ensemble_strategies.items():\n",
    "    pred_clipped = np.clip(pred, y_clean.min(), y_clean.max())\n",
    "    ic50_strategy = 10 ** (9 - pred_clipped)\n",
    "    ic50_strategy = np.clip(ic50_strategy, 0.1, 100000)\n",
    "    \n",
    "    submission_strategy = pd.DataFrame({\n",
    "        \"ID\": df_test[\"ID\"],\n",
    "        \"ASK1_IC50_nM\": ic50_strategy\n",
    "    })\n",
    "    \n",
    "    filename = f\"submit_conservative_{strategy_name}.csv\"\n",
    "    submission_strategy.to_csv(output_dir + filename, index=False)\n",
    "    print(f\"  {filename} 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
