{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a291b93d-190c-4f90-a718-518e96ac54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/merged_pubchem_chembl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d0948-b6a3-4e20-8e6e-d2469dea087d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593424e-72f5-4eaa-8b9a-0eeaeb3fc4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b201f923-f189-48b0-9b78-13da1797758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 빠른 상관관계 개선 시작!\n",
      "🧪 분자 기술자 계산...\n",
      "데이터 크기: 1960\n",
      "🧹 아웃라이어 제거...\n",
      "아웃라이어 제거: 1960 → 1959\n",
      "📊 고급 데이터 전처리...\n",
      "🎯 타겟 변환 최적화...\n",
      "🤖 고성능 모델 훈련...\n",
      "🏋️ 모델 훈련 중...\n",
      "  rf 훈련 중...\n",
      "  xgb 훈련 중...\n",
      "  lgb 훈련 중...\n",
      "  gb 훈련 중...\n",
      "  et 훈련 중...\n",
      "\n",
      "📊 개별 모델 성능:\n",
      "  rf (변환): 0.4054 (A=1.000, B=0.676)\n",
      "  xgb (변환): 0.3889 (A=1.000, B=0.648)\n",
      "  lgb (변환): 0.3973 (A=1.000, B=0.662)\n",
      "  gb (변환): 0.3863 (A=1.000, B=0.644)\n",
      "  et (변환): 0.4092 (A=1.000, B=0.682)\n",
      "\n",
      "🎭 상관관계 최적화 앙상블...\n",
      "선택된 모델: ['et', 'rf', 'lgb', 'xgb']\n",
      "최적 가중치: {'et': np.float64(0.7468461281871787), 'rf': np.float64(0.008466055649200789), 'lgb': np.float64(0.2446878161636207), 'xgb': np.float64(0.0)}\n",
      "\n",
      "🎉 최종 앙상블 성능:\n",
      "   Competition Score: 0.4104\n",
      "   RMSE: 0.8591\n",
      "   A (Normalized RMSE): 1.0000\n",
      "   B (Correlation²): 0.6840\n",
      "   pIC50 상관관계: 0.8271 (p=1.308e-99)\n",
      "\n",
      "🔮 테스트 데이터 예측 (수정)...\n",
      "특성 순서: ['MolWt', 'MolWt', 'LogP', 'LogP', 'TPSA', 'TPSA', 'NumRotatableBonds', 'NumRotatableBonds', 'NumHAcceptors', 'NumHAcceptors', 'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT', 'source_chembl', 'source_pubchem']\n",
      "유효한 테스트 데이터: 127/127\n",
      "테스트 데이터 형태: (127, 17)\n",
      "훈련 데이터 형태: (1567, 17)\n",
      "✅ 특성 순서 일치\n",
      "🔮 모델별 예측 중...\n",
      "  et: 가중치 0.7468\n",
      "    et: 변환 모델 사용\n",
      "  rf: 가중치 0.0085\n",
      "    rf: 변환 모델 사용\n",
      "  lgb: 가중치 0.2447\n",
      "    lgb: 변환 모델 사용\n",
      "  xgb: 가중치 0.0000\n",
      "    xgb: 변환 모델 사용\n",
      "\n",
      "📊 예측값 통계:\n",
      "  log_IC50 범위: -7.871 ~ -5.008\n",
      "  IC50 (nM) 범위: 0.0 ~ 9.8\n",
      "  IC50 (nM) 중간값: 0.7\n",
      "\n",
      "✅ fixed_correlation_submission.csv 생성!\n",
      "📊 예측값 수: 127\n",
      "\n",
      "==================================================\n",
      "🏆 최종 성능 요약\n",
      "==================================================\n",
      "검증 Competition Score: 0.4104\n",
      "A (Normalized RMSE): 1.0000\n",
      "B (Correlation²): 0.6840\n",
      "pIC50 상관관계: 0.8271\n",
      "성능 개선: 0.0104 (기준 0.4 대비)\n",
      "\n",
      "📊 모델 기여도:\n",
      "  et: 74.7%\n",
      "  rf: 0.8%\n",
      "  lgb: 24.5%\n",
      "  xgb: 0.0%\n",
      "==================================================\n",
      "✅ 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "빠른 상관관계 개선 전략 (기존 코드에 추가)\n",
    "- 타겟 변환 최적화\n",
    "- 아웃라이어 제거\n",
    "- 특성 스케일링 개선\n",
    "- 앙상블 가중치 최적화\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compute_safe_descriptors(smiles):\n",
    "    \"\"\"기존과 동일한 안전한 기술자 계산\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan] * 10\n",
    "    \n",
    "    try:\n",
    "        return [\n",
    "            Descriptors.MolWt(mol),\n",
    "            Descriptors.MolLogP(mol),\n",
    "            Descriptors.TPSA(mol),\n",
    "            Descriptors.NumRotatableBonds(mol),\n",
    "            Descriptors.NumHAcceptors(mol),\n",
    "            Descriptors.NumHDonors(mol),\n",
    "            Descriptors.NumAromaticRings(mol),\n",
    "            Descriptors.RingCount(mol),\n",
    "            Descriptors.NumHeteroatoms(mol),\n",
    "            Descriptors.BertzCT(mol)\n",
    "        ]\n",
    "    except:\n",
    "        return [np.nan] * 10\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    \"\"\"대회 평가 점수\"\"\"\n",
    "    try:\n",
    "        ic50_true = 10 ** (y_true + 6)\n",
    "        ic50_pred = 10 ** (y_pred + 6)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(ic50_true, ic50_pred))\n",
    "        A = min(rmse / np.mean(ic50_true), 1)\n",
    "        \n",
    "        pic50_true = -y_true\n",
    "        pic50_pred = -y_pred\n",
    "        correlation, _ = pearsonr(pic50_true, pic50_pred)\n",
    "        B = correlation ** 2\n",
    "        \n",
    "        score = 0.4 * (1 - A) + 0.6 * B\n",
    "        return score, A, B\n",
    "    except:\n",
    "        return 0.0, 1.0, 0.0\n",
    "\n",
    "def remove_outliers(X, y, method='iqr', factor=1.5):\n",
    "    \"\"\"아웃라이어 제거\"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "    else:  # zscore\n",
    "        z_scores = np.abs((y - y.mean()) / y.std())\n",
    "        mask = z_scores < factor\n",
    "    \n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def optimize_ensemble_weights(models, X_val, y_val):\n",
    "    \"\"\"상관관계 최적화를 위한 앙상블 가중치 계산\"\"\"\n",
    "    \n",
    "    # 각 모델의 예측값\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(X_val)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions).T  # (n_samples, n_models)\n",
    "    \n",
    "    def objective(weights):\n",
    "        # 가중 평균 예측\n",
    "        ensemble_pred = np.average(predictions, axis=1, weights=weights)\n",
    "        \n",
    "        # pIC50 상관관계 (B 점수)\n",
    "        pic50_true = -y_val\n",
    "        pic50_pred = -ensemble_pred\n",
    "        \n",
    "        try:\n",
    "            correlation, _ = pearsonr(pic50_true, pic50_pred)\n",
    "            return -(correlation ** 2)  # 최대화를 위해 음수\n",
    "        except:\n",
    "            return -0.0\n",
    "    \n",
    "    # 제약조건: 가중치 합 = 1, 모든 가중치 >= 0\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = [(0, 1) for _ in range(len(models))]\n",
    "    \n",
    "    # 초기값: 균등 가중치\n",
    "    initial_weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP', \n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result.x if result.success else initial_weights\n",
    "\n",
    "def create_polynomial_features(X, degree=2, interaction_only=True):\n",
    "    \"\"\"다항식 특성 생성 (선택적)\"\"\"\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, \n",
    "                             include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    return X_poly, poly\n",
    "\n",
    "# 메인 코드\n",
    "print(\"🚀 빠른 상관관계 개선 시작!\")\n",
    "\n",
    "# 1. 데이터 로드\n",
    "df_train = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/merged_pubchem_chembl.csv\")\n",
    "df_test = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/test.csv\")\n",
    "\n",
    "# SMILES 컬럼 찾기\n",
    "smiles_col = None\n",
    "for col in df_train.columns:\n",
    "    if 'smiles' in col.lower():\n",
    "        smiles_col = col\n",
    "        break\n",
    "\n",
    "# 2. 기술자 계산\n",
    "print(\"🧪 분자 기술자 계산...\")\n",
    "descriptor_results = []\n",
    "for smiles in df_train[smiles_col]:\n",
    "    result = compute_safe_descriptors(smiles)\n",
    "    descriptor_results.append(result)\n",
    "\n",
    "descriptor_df = pd.DataFrame(descriptor_results, columns=[\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT'\n",
    "])\n",
    "\n",
    "# 출처 정보\n",
    "if 'source' in df_train.columns:\n",
    "    df_train = pd.get_dummies(df_train, columns=[\"source\"])\n",
    "else:\n",
    "    df_train['source_chembl'] = 1\n",
    "    df_train['source_pubchem'] = 0\n",
    "\n",
    "# 3. 데이터 결합\n",
    "df_combined = pd.concat([df_train.reset_index(drop=True), descriptor_df], axis=1)\n",
    "\n",
    "# 4. 특성 선택 및 데이터 준비\n",
    "features = [\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT',\n",
    "    'source_chembl', 'source_pubchem'\n",
    "]\n",
    "\n",
    "X = df_combined[features]\n",
    "y = df_combined['log_IC50']\n",
    "\n",
    "# NaN 제거\n",
    "mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "X_clean = X[mask]\n",
    "y_clean = y[mask]\n",
    "\n",
    "print(f\"데이터 크기: {len(X_clean)}\")\n",
    "\n",
    "# 5. 아웃라이어 제거\n",
    "print(\"🧹 아웃라이어 제거...\")\n",
    "X_no_outliers, y_no_outliers = remove_outliers(X_clean, y_clean, method='iqr', factor=2.0)\n",
    "print(f\"아웃라이어 제거: {len(X_clean)} → {len(X_no_outliers)}\")\n",
    "\n",
    "# 6. 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_no_outliers, y_no_outliers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. 고급 스케일링\n",
    "print(\"📊 고급 데이터 전처리...\")\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 8. 타겟 변환 (선택적)\n",
    "print(\"🎯 타겟 변환 최적화...\")\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_val_transformed = target_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 9. 다양한 모델 훈련\n",
    "print(\"🤖 고성능 모델 훈련...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Random Forest (상관관계 최적화)\n",
    "models['rf'] = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=25,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# XGBoost (상관관계 특화)\n",
    "models['xgb'] = xgb.XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "models['lgb'] = lgb.LGBMRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=100,\n",
    "    min_child_samples=10,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Gradient Boosting\n",
    "models['gb'] = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "models['et'] = ExtraTreesRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 10. 모델 훈련 (두 가지 타겟으로)\n",
    "print(\"🏋️ 모델 훈련 중...\")\n",
    "\n",
    "trained_models_original = {}\n",
    "trained_models_transformed = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  {name} 훈련 중...\")\n",
    "    \n",
    "    # 원본 타겟으로 훈련\n",
    "    model_orig = type(model)(**model.get_params())\n",
    "    model_orig.fit(X_train_scaled, y_train)\n",
    "    trained_models_original[name] = model_orig\n",
    "    \n",
    "    # 변환된 타겟으로 훈련\n",
    "    model_trans = type(model)(**model.get_params())\n",
    "    model_trans.fit(X_train_scaled, y_train_transformed)\n",
    "    trained_models_transformed[name] = model_trans\n",
    "\n",
    "# 11. 개별 모델 성능 평가\n",
    "print(\"\\n📊 개별 모델 성능:\")\n",
    "\n",
    "model_scores = {}\n",
    "model_predictions = {}\n",
    "\n",
    "for name in models.keys():\n",
    "    # 원본 타겟 모델\n",
    "    pred_orig = trained_models_original[name].predict(X_val_scaled)\n",
    "    score_orig, A_orig, B_orig = competition_score(y_val, pred_orig)\n",
    "    \n",
    "    # 변환된 타겟 모델 (역변환)\n",
    "    pred_trans_raw = trained_models_transformed[name].predict(X_val_scaled)\n",
    "    pred_trans = target_transformer.inverse_transform(pred_trans_raw.reshape(-1, 1)).ravel()\n",
    "    score_trans, A_trans, B_trans = competition_score(y_val, pred_trans)\n",
    "    \n",
    "    # 더 좋은 점수 선택\n",
    "    if score_trans > score_orig:\n",
    "        model_scores[name] = score_trans\n",
    "        model_predictions[name] = pred_trans\n",
    "        print(f\"  {name} (변환): {score_trans:.4f} (A={A_trans:.3f}, B={B_trans:.3f})\")\n",
    "    else:\n",
    "        model_scores[name] = score_orig\n",
    "        model_predictions[name] = pred_orig\n",
    "        print(f\"  {name} (원본): {score_orig:.4f} (A={A_orig:.3f}, B={B_orig:.3f})\")\n",
    "\n",
    "# 12. 상관관계 최적화 앙상블\n",
    "print(\"\\n🎭 상관관계 최적화 앙상블...\")\n",
    "\n",
    "# 모델 리스트 (성능 순으로 정렬)\n",
    "sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_models = [name for name, score in sorted_models[:4]]  # 상위 4개\n",
    "\n",
    "print(f\"선택된 모델: {top_models}\")\n",
    "\n",
    "# 최적 가중치 계산\n",
    "selected_models = []\n",
    "for name in top_models:\n",
    "    if model_scores[name] > model_scores[name.replace('_transformed', '_original')]:\n",
    "        selected_models.append(trained_models_transformed[name])\n",
    "    else:\n",
    "        selected_models.append(trained_models_original[name])\n",
    "\n",
    "optimal_weights = optimize_ensemble_weights(selected_models, X_val_scaled, y_val)\n",
    "print(f\"최적 가중치: {dict(zip(top_models, optimal_weights))}\")\n",
    "\n",
    "# 13. 최종 앙상블 예측\n",
    "ensemble_pred = np.zeros(len(X_val_scaled))\n",
    "for i, (name, weight) in enumerate(zip(top_models, optimal_weights)):\n",
    "    pred = model_predictions[name]\n",
    "    ensemble_pred += weight * pred\n",
    "\n",
    "# 14. 최종 성능 평가\n",
    "final_score, final_A, final_B = competition_score(y_val, ensemble_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "\n",
    "print(f\"\\n🎉 최종 앙상블 성능:\")\n",
    "print(f\"   Competition Score: {final_score:.4f}\")\n",
    "print(f\"   RMSE: {final_rmse:.4f}\")\n",
    "print(f\"   A (Normalized RMSE): {final_A:.4f}\")\n",
    "print(f\"   B (Correlation²): {final_B:.4f}\")\n",
    "\n",
    "# 상관관계 세부 분석\n",
    "pic50_true = -y_val\n",
    "pic50_pred = -ensemble_pred\n",
    "correlation, p_value = pearsonr(pic50_true, pic50_pred)\n",
    "print(f\"   pIC50 상관관계: {correlation:.4f} (p={p_value:.3e})\")\n",
    "\n",
    "# 빠른 수정: 기존 코드에서 테스트 데이터 부분만 교체\n",
    "\n",
    "# 15. 테스트 데이터 예측 (수정된 버전)\n",
    "print(\"\\n🔮 테스트 데이터 예측 (수정)...\")\n",
    "\n",
    "# 테스트 데이터 기술자 계산\n",
    "test_descriptors = []\n",
    "for smiles in df_test[smiles_col]:\n",
    "    result = compute_safe_descriptors(smiles)\n",
    "    test_descriptors.append(result)\n",
    "\n",
    "test_desc_df = pd.DataFrame(test_descriptors, columns=[\n",
    "    'MolWt', 'LogP', 'TPSA', 'NumRotatableBonds', 'NumHAcceptors',\n",
    "    'NumHDonors', 'NumAromaticRings', 'RingCount', 'NumHeteroatoms', 'BertzCT'\n",
    "])\n",
    "\n",
    "# 출처 정보 추가\n",
    "test_desc_df['source_chembl'] = 0\n",
    "test_desc_df['source_pubchem'] = 1\n",
    "\n",
    "# 훈련에 사용된 특성과 정확히 같은 순서로 데이터 준비\n",
    "features_ordered = X_train.columns.tolist()  # 훈련 데이터의 컬럼 순서\n",
    "print(f\"특성 순서: {features_ordered}\")\n",
    "\n",
    "# 테스트 데이터를 같은 순서로 정렬\n",
    "X_test_ordered = test_desc_df[features_ordered].copy()\n",
    "\n",
    "# 유효한 테스트 데이터만 선택\n",
    "test_mask = ~X_test_ordered.isnull().any(axis=1)\n",
    "X_test_clean = X_test_ordered[test_mask]\n",
    "test_ids = df_test[test_mask]['ID']\n",
    "\n",
    "print(f\"유효한 테스트 데이터: {len(X_test_clean)}/{len(df_test)}\")\n",
    "print(f\"테스트 데이터 형태: {X_test_clean.shape}\")\n",
    "print(f\"훈련 데이터 형태: {X_train.shape}\")\n",
    "\n",
    "# 컬럼 순서 재확인\n",
    "if list(X_test_clean.columns) == list(X_train.columns):\n",
    "    print(\"✅ 특성 순서 일치\")\n",
    "else:\n",
    "    print(\"❌ 특성 순서 불일치 - 강제 정렬\")\n",
    "    X_test_clean = X_test_clean[X_train.columns]\n",
    "\n",
    "# 스케일링\n",
    "X_test_scaled = scaler.transform(X_test_clean)\n",
    "\n",
    "# 앙상블 예측\n",
    "test_ensemble_pred = np.zeros(len(X_test_scaled))\n",
    "\n",
    "print(\"🔮 모델별 예측 중...\")\n",
    "for i, (name, weight) in enumerate(zip(top_models, optimal_weights)):\n",
    "    print(f\"  {name}: 가중치 {weight:.4f}\")\n",
    "    \n",
    "    if model_scores[name] == max([model_scores[n] for n in [name] if n in model_scores]):\n",
    "        # 최고 성능 모델 선택 로직 수정\n",
    "        try:\n",
    "            # 변환된 모델이 더 좋은 경우\n",
    "            pred_raw = trained_models_transformed[name].predict(X_test_scaled)\n",
    "            pred = target_transformer.inverse_transform(pred_raw.reshape(-1, 1)).ravel()\n",
    "            print(f\"    {name}: 변환 모델 사용\")\n",
    "        except:\n",
    "            # 원본 모델 사용\n",
    "            pred = trained_models_original[name].predict(X_test_scaled)\n",
    "            print(f\"    {name}: 원본 모델 사용\")\n",
    "    else:\n",
    "        # 원본 모델 사용\n",
    "        pred = trained_models_original[name].predict(X_test_scaled)\n",
    "        print(f\"    {name}: 원본 모델 사용\")\n",
    "    \n",
    "    test_ensemble_pred += weight * pred\n",
    "\n",
    "# IC50 변환\n",
    "ic50_pred_nM = 10 ** (test_ensemble_pred + 6)\n",
    "\n",
    "# 예측값 범위 확인\n",
    "print(f\"\\n📊 예측값 통계:\")\n",
    "print(f\"  log_IC50 범위: {test_ensemble_pred.min():.3f} ~ {test_ensemble_pred.max():.3f}\")\n",
    "print(f\"  IC50 (nM) 범위: {ic50_pred_nM.min():.1f} ~ {ic50_pred_nM.max():.1f}\")\n",
    "print(f\"  IC50 (nM) 중간값: {np.median(ic50_pred_nM):.1f}\")\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'ASK1_IC50_nM': ic50_pred_nM\n",
    "})\n",
    "\n",
    "submission.to_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/fixed_correlation_submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✅ fixed_correlation_submission.csv 생성!\")\n",
    "print(f\"📊 예측값 수: {len(submission)}\")\n",
    "\n",
    "# 성능 요약\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"🏆 최종 성능 요약\")\n",
    "print(\"=\"*50)\n",
    "print(f\"검증 Competition Score: {final_score:.4f}\")\n",
    "print(f\"A (Normalized RMSE): {final_A:.4f}\")\n",
    "print(f\"B (Correlation²): {final_B:.4f}\")\n",
    "print(f\"pIC50 상관관계: {correlation:.4f}\")\n",
    "print(f\"성능 개선: {final_score - 0.4:.4f} (기준 0.4 대비)\")\n",
    "\n",
    "# 모델 기여도\n",
    "print(f\"\\n📊 모델 기여도:\")\n",
    "for name, weight in zip(top_models, optimal_weights):\n",
    "    print(f\"  {name}: {weight:.1%}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"✅ 작업 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f7620-0cfe-4eb8-acfe-c5aa682c7715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c8e936a-86af-4a68-8665-55bf26c7c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ensemble_stable.csv 생성 완료\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c6fb2-538f-46b8-95a9-fbe26aff0870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8770872f-afde-4ce7-b272-0455e0369fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MAP3K5(ASK1) IC50 예측 - 고급 앙상블 최적화\n",
    "- Multi-model Stacking with Optuna\n",
    "- Advanced Feature Engineering\n",
    "- Quantile Matching & Blending\n",
    "\"\"\"\n",
    "\n",
    "# ======================== 1. 라이브러리 설치 ========================\n",
    "# !pip install optuna lightgbm xgboost catboost rdkit-pypi -q\n",
    "\n",
    "# ======================== 2. 필수 라이브러리 ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import rankdata, pearsonr\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, Lipinski, Crippen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDKit 경고 메시지 완전 제거\n",
    "import os\n",
    "os.environ['RDK_ERROR_STREAM'] = '/dev/null'\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Optuna 로깅 레벨 설정\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ======================== 3. 고급 피처 엔지니어링 ========================\n",
    "\n",
    "def calculate_advanced_features(smiles):\n",
    "    \"\"\"확장된 분자 기술자 계산 - 안정화 버전\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 기본 기술자 (안정적)\n",
    "        try:\n",
    "            features['MolWt'] = Descriptors.MolWt(mol)\n",
    "            features['LogP'] = Descriptors.MolLogP(mol)\n",
    "            features['TPSA'] = Descriptors.TPSA(mol)\n",
    "            features['NumRotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "            features['NumHAcceptors'] = Descriptors.NumHAcceptors(mol)\n",
    "            features['NumHDonors'] = Descriptors.NumHDonors(mol)\n",
    "            features['NumAromaticRings'] = Descriptors.NumAromaticRings(mol)\n",
    "            features['RingCount'] = Descriptors.RingCount(mol)\n",
    "            features['NumHeteroatoms'] = Descriptors.NumHeteroatoms(mol)\n",
    "            features['HeavyAtomCount'] = Descriptors.HeavyAtomCount(mol)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 고급 기술자 (버전별 호환성 체크)\n",
    "        try:\n",
    "            features['BertzCT'] = Descriptors.BertzCT(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['Chi0'] = Descriptors.Chi0(mol)\n",
    "            features['Chi1'] = Descriptors.Chi1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['HallKierAlpha'] = Descriptors.HallKierAlpha(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['Kappa1'] = Descriptors.Kappa1(mol)\n",
    "            features['Kappa2'] = Descriptors.Kappa2(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['FractionCsp3'] = Descriptors.FractionCsp3(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumSaturatedRings'] = Descriptors.NumSaturatedRings(mol)\n",
    "            features['NumAliphaticRings'] = Descriptors.NumAliphaticRings(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['MolMR'] = Crippen.MolMR(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['BalabanJ'] = Descriptors.BalabanJ(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # VSA 기술자들\n",
    "        try:\n",
    "            features['PEOE_VSA1'] = Descriptors.PEOE_VSA1(mol)\n",
    "            features['PEOE_VSA2'] = Descriptors.PEOE_VSA2(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['SMR_VSA1'] = Descriptors.SMR_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['SlogP_VSA1'] = Descriptors.SlogP_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['EState_VSA1'] = Descriptors.EState_VSA1(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # 약물성 지표\n",
    "        try:\n",
    "            features['QED'] = Descriptors.qed(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumHeavyAtoms'] = Lipinski.NumHeavyAtoms(mol)\n",
    "        except: pass\n",
    "        \n",
    "        try:\n",
    "            features['NumAliphaticCarbocycles'] = Lipinski.NumAliphaticCarbocycles(mol)\n",
    "            features['NumAliphaticHeterocycles'] = Lipinski.NumAliphaticHeterocycles(mol)\n",
    "            features['NumAromaticCarbocycles'] = Lipinski.NumAromaticCarbocycles(mol)\n",
    "            features['NumAromaticHeterocycles'] = Lipinski.NumAromaticHeterocycles(mol)\n",
    "            features['NumSaturatedCarbocycles'] = Lipinski.NumSaturatedCarbocycles(mol)\n",
    "            features['NumSaturatedHeterocycles'] = Lipinski.NumSaturatedHeterocycles(mol)\n",
    "        except: pass\n",
    "        \n",
    "        # 추가 안정적인 기술자들\n",
    "        try:\n",
    "            features['NumRadicalElectrons'] = Descriptors.NumRadicalElectrons(mol)\n",
    "            features['NumValenceElectrons'] = Descriptors.NumValenceElectrons(mol)\n",
    "        except: pass\n",
    "        \n",
    "        return features if features else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def get_morgan_fingerprint_features(smiles, radius=2, n_bits=1024):\n",
    "    \"\"\"Morgan Fingerprint를 피처로 변환 - 새로운 API 사용\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)\n",
    "    \n",
    "    try:\n",
    "        # 새로운 API 사용 (RDKit 2022+)\n",
    "        fp_gen = AllChem.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "        fp = fp_gen.GetFingerprint(mol)\n",
    "        return np.array(fp)\n",
    "    except:\n",
    "        # 구버전 API 폴백\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "\n",
    "# ======================== 4. 데이터 로드 및 전처리 ========================\n",
    "\n",
    "print(\"📊 데이터 로드 및 전처리 시작...\")\n",
    "\n",
    "# 데이터 로드\n",
    "df_train = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/chembl_processed_rescaled.csv\")\n",
    "df_test = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/test.csv\")\n",
    "\n",
    "# 데이터 클리닝\n",
    "df_train = df_train[df_train[\"IC50\"] > 0].copy()\n",
    "df_train = df_train[(df_train[\"IC50\"] >= 0.1) & (df_train[\"IC50\"] <= 1e5)].copy()\n",
    "\n",
    "# pIC50 계산\n",
    "df_train[\"pIC50\"] = 9 - np.log10(df_train[\"IC50\"])\n",
    "\n",
    "# SMILES 컬럼 찾기\n",
    "smiles_col = 'Smiles' if 'Smiles' in df_train.columns else 'smiles'\n",
    "smiles_col_test = 'Smiles' if 'Smiles' in df_test.columns else 'smiles'\n",
    "\n",
    "# 고급 피처 추출 (학습 데이터)\n",
    "print(\"🧪 고급 피처 추출 중...\")\n",
    "train_features_list = []\n",
    "for idx, smiles in enumerate(df_train[smiles_col]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"  처리 중: {idx}/{len(df_train)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        train_features_list.append(features)\n",
    "    else:\n",
    "        train_features_list.append({})\n",
    "\n",
    "train_features_df = pd.DataFrame(train_features_list)\n",
    "\n",
    "# Morgan Fingerprint 추가 (차원 축소)\n",
    "print(\"🔬 Morgan Fingerprint 계산...\")\n",
    "n_fp_bits = 256  # 메모리 효율을 위해 축소\n",
    "train_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_train[smiles_col]])\n",
    "\n",
    "# PCA로 차원 축소\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "train_fp_pca = pca.fit_transform(train_fp_array)\n",
    "train_fp_df = pd.DataFrame(train_fp_pca, columns=[f'FP_PC{i+1}' for i in range(50)])\n",
    "\n",
    "# 모든 피처 결합\n",
    "X_full = pd.concat([train_features_df, train_fp_df], axis=1)\n",
    "y_full = df_train[\"pIC50\"]\n",
    "\n",
    "# NaN 처리\n",
    "X_full = X_full.fillna(X_full.median())\n",
    "valid_mask = ~(X_full.isnull().any(axis=1) | y_full.isnull())\n",
    "X_clean = X_full[valid_mask]\n",
    "y_clean = y_full[valid_mask]\n",
    "\n",
    "print(f\"✅ 유효 데이터: {len(X_clean)} samples, {X_clean.shape[1]} features\")\n",
    "\n",
    "# ======================== 5. 다중 스케일링 전략 ========================\n",
    "\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'quantile': QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "}\n",
    "\n",
    "X_scaled = {}\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled[name] = scaler.fit_transform(X_clean)\n",
    "    print(f\"  {name} 스케일링 완료\")\n",
    "\n",
    "# 학습/검증 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled['robust'], y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ======================== 6. Optuna 다중 모델 최적화 ========================\n",
    "\n",
    "print(\"\\n🎯 Optuna 하이퍼파라미터 최적화 시작...\")\n",
    "\n",
    "def create_objective(model_type, X_train, y_train, cv_folds=5):\n",
    "    \"\"\"각 모델별 Optuna 목적 함수 생성\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'lgb':\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'verbosity': -1,\n",
    "                'n_estimators': 500,  # 줄여서 속도 향상\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            }\n",
    "            model_class = lgb.LGBMRegressor\n",
    "            \n",
    "        elif model_type == 'xgb':\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': 500,  # 줄여서 속도 향상\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            }\n",
    "            model_class = xgb.XGBRegressor\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': 500,  # 줄여서 속도 향상\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0.0, 10.0),\n",
    "                'verbose': False,\n",
    "            }\n",
    "            model_class = cb.CatBoostRegressor\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_float('max_features', 0.3, 1.0),\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42,\n",
    "            }\n",
    "            model_class = RandomForestRegressor\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        rmse_list = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_fold_train = X_train[train_idx]\n",
    "            X_fold_val = X_train[val_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]\n",
    "            \n",
    "            model = model_class(**params)\n",
    "            \n",
    "            # 간단한 학습 (early stopping 없이)\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            preds = model.predict(X_fold_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "            rmse_list.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_list)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "# 각 모델 최적화 - 속도를 위해 trials 수 줄임\n",
    "best_params = {}\n",
    "studies = {}\n",
    "\n",
    "for model_type in ['lgb', 'xgb', 'catboost', 'rf']:\n",
    "    print(f\"\\n  {model_type.upper()} 최적화 중...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(\n",
    "        create_objective(model_type, X_train, y_train),\n",
    "        n_trials=10,  # 속도를 위해 줄임 (실제로는 50-100 권장)\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    best_params[model_type] = study.best_params\n",
    "    studies[model_type] = study\n",
    "    print(f\"    Best RMSE: {study.best_value:.4f}\")\n",
    "\n",
    "# ======================== 7. 최적화된 모델 학습 ========================\n",
    "\n",
    "print(\"\\n🤖 최적화된 모델 학습...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# LightGBM\n",
    "models['lgb'] = lgb.LGBMRegressor(**best_params['lgb'], n_estimators=1000, verbosity=-1)\n",
    "try:\n",
    "    models['lgb'].fit(X_train, y_train, \n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "except:\n",
    "    # 폴백: callbacks 없이\n",
    "    models['lgb'].fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "models['xgb'] = xgb.XGBRegressor(**best_params['xgb'], n_estimators=1000)\n",
    "try:\n",
    "    # 새 버전 (XGBoost 2.0+)\n",
    "    models['xgb'].set_params(early_stopping_rounds=50)\n",
    "    models['xgb'].fit(X_train, y_train,\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      verbose=False)\n",
    "except:\n",
    "    try:\n",
    "        # 구 버전\n",
    "        models['xgb'].fit(X_train, y_train,\n",
    "                          eval_set=[(X_val, y_val)],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose=False)\n",
    "    except:\n",
    "        # early stopping 없이\n",
    "        models['xgb'].fit(X_train, y_train)\n",
    "\n",
    "# CatBoost\n",
    "models['catboost'] = cb.CatBoostRegressor(**best_params['catboost'], iterations=1000)\n",
    "try:\n",
    "    models['catboost'].fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False, early_stopping_rounds=50)\n",
    "except:\n",
    "    models['catboost'].fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Random Forest\n",
    "models['rf'] = RandomForestRegressor(**best_params['rf'])\n",
    "models['rf'].fit(X_train, y_train)\n",
    "\n",
    "# Extra Trees (고정 파라미터)\n",
    "models['extra'] = ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1)\n",
    "models['extra'].fit(X_train, y_train)\n",
    "\n",
    "# Neural Network\n",
    "models['mlp'] = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "models['mlp'].fit(X_train, y_train)\n",
    "\n",
    "# ======================== 8. 모델 평가 및 가중치 최적화 ========================\n",
    "\n",
    "print(\"\\n📊 모델 성능 평가...\")\n",
    "\n",
    "val_predictions = {}\n",
    "val_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    \n",
    "    # pIC50 상관관계\n",
    "    corr, _ = pearsonr(y_val, pred)\n",
    "    \n",
    "    val_predictions[name] = pred\n",
    "    val_scores[name] = {'rmse': rmse, 'r2': r2, 'corr': corr}\n",
    "    \n",
    "    print(f\"  {name:10s}: RMSE={rmse:.4f}, R²={r2:.4f}, Corr={corr:.4f}\")\n",
    "\n",
    "# 최적 가중치 찾기\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def ensemble_objective(weights):\n",
    "    ensemble_pred = np.zeros(len(y_val))\n",
    "    for i, name in enumerate(models.keys()):\n",
    "        ensemble_pred += weights[i] * val_predictions[name]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "    return rmse\n",
    "\n",
    "# 제약조건: 가중치 합 = 1, 모든 가중치 >= 0\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1) for _ in range(len(models))]\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "result = minimize(ensemble_objective, initial_weights, \n",
    "                 method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "optimal_weights = result.x\n",
    "print(f\"\\n✅ 최적 가중치:\")\n",
    "for name, weight in zip(models.keys(), optimal_weights):\n",
    "    if weight > 0.01:\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "\n",
    "# ======================== 9. 전체 데이터로 재학습 ========================\n",
    "\n",
    "print(\"\\n🔄 전체 데이터로 모델 재학습...\")\n",
    "\n",
    "# 전체 데이터 스케일링\n",
    "X_full_scaled = scalers['robust'].fit_transform(X_clean)\n",
    "\n",
    "models_full = {}\n",
    "\n",
    "# 각 모델을 전체 데이터로 재학습\n",
    "for name in models.keys():\n",
    "    if name == 'lgb':\n",
    "        models_full[name] = lgb.LGBMRegressor(**best_params['lgb'], n_estimators=1200, verbosity=-1)\n",
    "    elif name == 'xgb':\n",
    "        models_full[name] = xgb.XGBRegressor(**best_params['xgb'], n_estimators=1200)\n",
    "    elif name == 'catboost':\n",
    "        models_full[name] = cb.CatBoostRegressor(**best_params['catboost'], iterations=1200, verbose=False)\n",
    "    elif name == 'rf':\n",
    "        models_full[name] = RandomForestRegressor(**best_params['rf'])\n",
    "    elif name == 'extra':\n",
    "        models_full[name] = ExtraTreesRegressor(n_estimators=600, max_depth=20, random_state=42, n_jobs=-1)\n",
    "    elif name == 'mlp':\n",
    "        models_full[name] = MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128, 64),\n",
    "            activation='relu',\n",
    "            max_iter=1500,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    models_full[name].fit(X_full_scaled, y_clean)\n",
    "    print(f\"  {name} 학습 완료\")\n",
    "\n",
    "# ======================== 10. 테스트 데이터 예측 ========================\n",
    "\n",
    "print(\"\\n🔮 테스트 데이터 예측...\")\n",
    "\n",
    "# 테스트 데이터 피처 추출\n",
    "test_features_list = []\n",
    "for idx, smiles in enumerate(df_test[smiles_col_test]):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"  처리 중: {idx}/{len(df_test)}\")\n",
    "    features = calculate_advanced_features(smiles)\n",
    "    if features:\n",
    "        test_features_list.append(features)\n",
    "    else:\n",
    "        test_features_list.append({})\n",
    "\n",
    "test_features_df = pd.DataFrame(test_features_list)\n",
    "\n",
    "# Morgan Fingerprint\n",
    "test_fp_array = np.array([get_morgan_fingerprint_features(s, n_bits=n_fp_bits) \n",
    "                          for s in df_test[smiles_col_test]])\n",
    "test_fp_pca = pca.transform(test_fp_array)\n",
    "test_fp_df = pd.DataFrame(test_fp_pca, columns=[f'FP_PC{i+1}' for i in range(50)])\n",
    "\n",
    "# 결합\n",
    "X_test_full = pd.concat([test_features_df, test_fp_df], axis=1)\n",
    "X_test_full = X_test_full.fillna(X_test_full.median())\n",
    "\n",
    "# 학습 데이터와 동일한 컬럼 순서 보장\n",
    "X_test_full = X_test_full[X_clean.columns]\n",
    "\n",
    "# 스케일링\n",
    "X_test_scaled = scalers['robust'].transform(X_test_full)\n",
    "\n",
    "# 각 모델로 예측\n",
    "test_predictions = {}\n",
    "for name, model in models_full.items():\n",
    "    test_predictions[name] = model.predict(X_test_scaled)\n",
    "    print(f\"  {name} 예측 완료\")\n",
    "\n",
    "# ======================== 11. Quantile Matching & 앙상블 ========================\n",
    "\n",
    "print(\"\\n🎭 Quantile Matching & 최종 앙상블...\")\n",
    "\n",
    "def quantile_match(source_pred, target_pred):\n",
    "    \"\"\"Quantile Matching으로 분포 정렬\"\"\"\n",
    "    sorted_target = np.sort(target_pred)\n",
    "    source_ranks = rankdata(source_pred, method='ordinal') - 1\n",
    "    source_ranks = np.clip(source_ranks, 0, len(sorted_target)-1).astype(int)\n",
    "    return sorted_target[source_ranks]\n",
    "\n",
    "# RF를 기준으로 다른 모델들 Quantile Matching\n",
    "base_pred = test_predictions['rf']\n",
    "matched_predictions = {'rf': base_pred}\n",
    "\n",
    "for name in ['lgb', 'xgb', 'catboost', 'extra', 'mlp']:\n",
    "    matched_predictions[name] = quantile_match(test_predictions[name], base_pred)\n",
    "\n",
    "# 가중 평균 앙상블\n",
    "ensemble_pred = np.zeros(len(X_test_scaled))\n",
    "for i, name in enumerate(models.keys()):\n",
    "    ensemble_pred += optimal_weights[i] * matched_predictions[name]\n",
    "\n",
    "# ======================== 12. 후처리 및 제출 파일 생성 ========================\n",
    "\n",
    "# 클리핑\n",
    "ensemble_pred = np.clip(ensemble_pred, y_clean.min(), y_clean.max())\n",
    "\n",
    "# IC50 역변환\n",
    "ic50_pred = 10 ** (9 - ensemble_pred)\n",
    "\n",
    "# 추가 후처리: 극단값 제한\n",
    "ic50_pred = np.clip(ic50_pred, 0.1, 100000)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": ic50_pred\n",
    "})\n",
    "\n",
    "# 저장 경로 설정\n",
    "output_dir = \"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/submissions/\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 저장\n",
    "submission.to_csv(output_dir + \"submit_advanced_ensemble.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎊 예측 완료!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"예측 통계:\")\n",
    "print(f\"  IC50 범위: {ic50_pred.min():.2f} ~ {ic50_pred.max():.2f} nM\")\n",
    "print(f\"  IC50 중간값: {np.median(ic50_pred):.2f} nM\")\n",
    "print(f\"  IC50 평균: {np.mean(ic50_pred):.2f} nM\")\n",
    "print(f\"  IC50 표준편차: {np.std(ic50_pred):.2f} nM\")\n",
    "print(\"\\n✅ 제출 파일 저장: submit_advanced_ensemble.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ======================== 13. 추가: Stacking 앙상블 (선택사항) ========================\n",
    "\n",
    "print(\"\\n🔥 보너스: Stacking 앙상블...\")\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# 베이스 모델들\n",
    "base_models = [\n",
    "    ('lgb', lgb.LGBMRegressor(**best_params['lgb'], n_estimators=500, verbosity=-1)),\n",
    "    ('xgb', xgb.XGBRegressor(**best_params['xgb'], n_estimators=500)),\n",
    "    ('rf', RandomForestRegressor(**best_params['rf'])),\n",
    "]\n",
    "\n",
    "# 메타 모델\n",
    "meta_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "\n",
    "# Stacking\n",
    "stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "stacking.fit(X_full_scaled, y_clean)\n",
    "\n",
    "# Stacking 예측\n",
    "stacking_pred = stacking.predict(X_test_scaled)\n",
    "stacking_pred = np.clip(stacking_pred, y_clean.min(), y_clean.max())\n",
    "ic50_stacking = 10 ** (9 - stacking_pred)\n",
    "ic50_stacking = np.clip(ic50_stacking, 0.1, 100000)\n",
    "\n",
    "# Stacking 제출 파일\n",
    "submission_stacking = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": ic50_stacking\n",
    "})\n",
    "submission_stacking.to_csv(output_dir + \"submit_stacking.csv\", index=False)\n",
    "\n",
    "print(\"✅ Stacking 제출 파일 저장: submit_stacking.csv\")\n",
    "\n",
    "# 최종 블렌딩 (앙상블 + Stacking)\n",
    "final_pred = 0.7 * ic50_pred + 0.3 * ic50_stacking\n",
    " \n",
    "submission_final = pd.DataFrame({\n",
    "    \"ID\": df_test[\"ID\"],\n",
    "    \"ASK1_IC50_nM\": final_pred\n",
    "})\n",
    "submission_final.to_csv(output_dir + \"submit_final_blend.csv\", index=False)\n",
    "\n",
    "print(\"✅ 최종 블렌딩 제출 파일 저장: submit_final_blend.csv\")\n",
    "print(\"\\n🏆 모든 작업 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22af310d-96a5-4b5f-a103-4b62ceff7591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>IC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL324340</td>\n",
       "      <td>Cc1ccc2oc(-c3cccc(N4C(=O)c5ccc(C(=O)O)cc5C4=O)...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.71</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.374</td>\n",
       "      <td>4.30202</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL109600</td>\n",
       "      <td>COc1ccccc1-c1ccc2oc(-c3ccc(OC)c(N4C(=O)c5ccc(C...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>119.17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>520.497</td>\n",
       "      <td>5.67780</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL357278</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4ccc(Cl)c(C(...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>543.011</td>\n",
       "      <td>4.27292</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL357119</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>468.623</td>\n",
       "      <td>2.32092</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL152968</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>516.667</td>\n",
       "      <td>4.26772</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Smiles  \\\n",
       "0  CHEMBL324340  Cc1ccc2oc(-c3cccc(N4C(=O)c5ccc(C(=O)O)cc5C4=O)...   \n",
       "1  CHEMBL109600  COc1ccccc1-c1ccc2oc(-c3ccc(OC)c(N4C(=O)c5ccc(C...   \n",
       "2  CHEMBL357278  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4ccc(Cl)c(C(...   \n",
       "3  CHEMBL357119  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...   \n",
       "4  CHEMBL152968  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...   \n",
       "\n",
       "   NumHDonors    TPSA  NumHAcceptors    MolWt     LogP  NumRotatableBonds  \\\n",
       "0         1.0  100.71            5.0  398.374  4.30202                3.0   \n",
       "1         1.0  119.17            7.0  520.497  5.67780                6.0   \n",
       "2         2.0   77.93            7.0  543.011  4.27292                8.0   \n",
       "3         2.0   77.93            7.0  468.623  2.32092               10.0   \n",
       "4         2.0   77.93            7.0  516.667  4.26772                9.0   \n",
       "\n",
       "   RingCount  HeavyAtomCount     IC50  \n",
       "0        5.0            30.0   2500.0  \n",
       "1        6.0            39.0   9000.0  \n",
       "2        4.0            36.0   4000.0  \n",
       "3        4.0            33.0  17000.0  \n",
       "4        5.0            37.0    180.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data2/project/2025summer/jjh0709/git/Jump-AI-2025/data/ChEMBL_IC50_30k_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6ae854-a301-4f51-b653-0d21005d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
